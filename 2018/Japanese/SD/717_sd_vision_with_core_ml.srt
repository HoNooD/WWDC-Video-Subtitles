
1
00:00:07,107 --> 00:00:16,250
(音楽)

2
00:00:18,752 --> 00:00:25,292
(拍手)

3
00:00:25,392 --> 00:00:28,228
WWDCへようこそ

4
00:00:28,328 --> 00:00:33,333
コーヒーで
一息入れたいところだと思いますが

5
00:00:33,433 --> 00:00:36,036
私の話をお楽しみください

6
00:00:36,170 --> 00:00:37,204
私はフランク

7
00:00:37,304 --> 00:00:41,175
Core MLと
Visionフレームワークを用いた―

8
00:00:41,275 --> 00:00:43,510
コンピュータビジョンについて
ご紹介します

9
00:00:44,311 --> 00:00:46,046
まず手始めに―

10
00:00:46,613 --> 00:00:52,486
画像分類のカスタマイズについて
お話しします

11
00:00:53,220 --> 00:00:55,656
次に物体認識

12
00:00:56,356 --> 00:01:01,895
最後にVisionの基本に関して
詳しくお伝えします

13
00:00:56,356 --> 00:01:01,895
最後にVisionの基本に関して
詳しくお伝えします

14
00:01:04,164 --> 00:01:06,099
画像分類については

15
00:01:07,267 --> 00:01:11,638
その利点を
すでにお聞きになりましたね

16
00:01:11,738 --> 00:01:17,311
花や果物を使ったデモを
ご覧いただきましたが

17
00:01:17,411 --> 00:01:20,747
ここでは より技術的な話をします

18
00:01:20,848 --> 00:01:27,354
例えば ロボットの部品を
扱う店を開いたとしましょう

19
00:01:27,487 --> 00:01:30,190
部品がたくさんあるので

20
00:01:30,290 --> 00:01:36,964
アプリケーションを用いて
部品を識別することにします

21
00:01:38,232 --> 00:01:41,068
そこで 分類器を
トレーニングします

22
00:01:41,635 --> 00:01:44,905
分類器が完成したら

23
00:01:45,005 --> 00:01:49,510
iOSアプリケーションを
構築します

24
00:01:50,043 --> 00:01:53,280
この作業を行っていく中で

25
00:01:53,380 --> 00:01:58,318
注意すべき点についても
後ほどお伝えします

26
00:02:00,854 --> 00:02:02,990
では トレーニングから説明します

27
00:02:03,290 --> 00:02:05,759
Create MLを用います

28
00:02:07,027 --> 00:02:10,430
まずは画像を用意しましょう

29
00:02:11,665 --> 00:02:17,070
画像を保存するフォルダ名が
分類ラベルになります

30
00:02:18,805 --> 00:02:23,043
画像データは
どれほど必要でしょうか？

31
00:02:23,977 --> 00:02:30,717
１つのカテゴリにつき
最低10件程度は必要ですが

32
00:02:31,151 --> 00:02:36,924
画像数が多いほど
分類結果は正確になります

33
00:02:38,325 --> 00:02:42,829
画像数のバランスにも
注意が必要です

34
00:02:42,930 --> 00:02:47,634
カテゴリによって
数に大きな差があると

35
00:02:47,734 --> 00:02:50,237
トレーニングは
うまくいきません

36
00:02:50,337 --> 00:02:53,707
同程度の数を準備しましょう

37
00:02:55,609 --> 00:02:59,713
augmentationという機能も
効果的で

38
00:03:00,013 --> 00:03:03,450
より安定したモデルに
なります

39
00:03:03,550 --> 00:03:06,653
ただし これを使うからといって

40
00:03:06,753 --> 00:03:10,991
画像が少なくても
済むわけではありませんよ

41
00:03:11,258 --> 00:03:15,896
augmentationは
画像に変化を加えます

42
00:03:15,996 --> 00:03:19,333
ぼかしたり 回転させたりして

43
00:03:19,433 --> 00:03:22,669
トレーニング用に
見た目を変えるのです

44
00:03:24,738 --> 00:03:27,774
トレーニングの仕組みを
見てみましょう

45
00:03:29,343 --> 00:03:31,912
転移学習という技術を使い

46
00:03:32,012 --> 00:03:35,916
Create MLの
分類器をトレーニングします

47
00:03:36,283 --> 00:03:40,521
これには学習済みの
モデルを用います

48
00:03:40,621 --> 00:03:45,058
数百万の画像で
数週間かけてトレーニングし

49
00:03:45,158 --> 00:03:48,862
その状態から始められます

50
00:03:50,364 --> 00:03:54,735
このモデルを使うと
画像の特徴を抽出して

51
00:03:54,835 --> 00:03:58,639
数値化することが
可能になります

52
00:03:59,373 --> 00:04:05,479
ここにデータを取り込んで
分類器をトレーニングすると

53
00:03:59,373 --> 00:04:05,479
ここにデータを取り込んで
分類器をトレーニングすると

54
00:04:05,579 --> 00:04:09,349
カスタムモデルが完成します

55
00:04:11,285 --> 00:04:14,855
この学習済みモデルに加えて

56
00:04:15,355 --> 00:04:20,293
Vision FeaturePrint.Sceneも
あります

57
00:04:20,994 --> 00:04:26,133
Create MLと連携させて
画像分類器をトレーニングできます

58
00:04:27,701 --> 00:04:30,571
大量のデータで
トレーニング済みで

59
00:04:31,371 --> 00:04:35,209
1000以上のカテゴリを
分類できます

60
00:04:35,309 --> 00:04:39,213
これは非常に便利ですよね

61
00:04:40,681 --> 00:04:44,785
すでに数年前から このモデルを

62
00:04:44,885 --> 00:04:48,856
写真アプリケーションで
使ってきました

63
00:04:49,957 --> 00:04:55,629
今後も改良は続けますが
注意すべき点があります

64
00:04:56,630 --> 00:04:59,299
最新版のモデルでも

65
00:04:59,399 --> 00:05:04,338
活用するには再トレーニングが
必要ということです

66
00:04:59,399 --> 00:05:04,338
活用するには再トレーニングが
必要ということです

67
00:05:04,571 --> 00:05:10,844
新しいモデルが登場した時のために
データをとっておき

68
00:05:10,944 --> 00:05:14,815
それを使って
再トレーニングしましょう

69
00:05:16,984 --> 00:05:20,120
この新しいVisionは―

70
00:05:21,922 --> 00:05:26,360
すでに デバイスに
搭載されています

71
00:05:26,460 --> 00:05:31,532
ディスクのフットプリントを
小さく抑えられるからです

72
00:05:32,032 --> 00:05:33,667
比べてみましょう

73
00:05:34,001 --> 00:05:38,772
最初は現在
一般的なモデルの１つである―

74
00:05:38,872 --> 00:05:44,111
ResNetで分類器を
トレーニングした際の容量です

75
00:05:44,411 --> 00:05:46,013
98MBです

76
00:05:47,314 --> 00:05:53,387
次に より小さいモデルの
SqueezeNetで試してみます

77
00:05:53,754 --> 00:05:59,193
５MBまで減りましたが
対応能力は限られます

78
00:05:59,493 --> 00:06:00,727
Visionは―

79
00:05:59,493 --> 00:06:00,727
Visionは―

80
00:06:01,728 --> 00:06:04,131
なんと１MB以下です

81
00:06:06,233 --> 00:06:11,505
すでに最適化されている点でも
とても優秀です

82
00:06:11,605 --> 00:06:17,444
ハードウェアやGPU
CPUとの連携も考慮しており

83
00:06:17,878 --> 00:06:21,682
我が社のデバイス上で
効率的に働きます

84
00:06:24,318 --> 00:06:25,986
トレーニングの方法は？

85
00:06:26,954 --> 00:06:31,291
Create MLが画像を読み込むと

86
00:06:31,391 --> 00:06:35,596
Vision FeaturePrint.Sceneと
連携します

87
00:06:36,530 --> 00:06:42,369
そして 分類器がトレーニングされ
Core MLモデルが作成されます

88
00:06:42,469 --> 00:06:44,104
だから小さいのです

89
00:06:45,506 --> 00:06:48,475
実際に画像を分類させる時

90
00:06:48,775 --> 00:06:52,446
必要なのは
画像とモデルだけです

91
00:06:52,746 --> 00:07:00,087
VisionとCore MLは
分類方法を学習しているので

92
00:06:52,746 --> 00:07:00,087
VisionとCore MLは
分類方法を学習しているので

93
00:07:00,187 --> 00:07:04,358
画像の分類結果が
出てくるというわけです

94
00:07:07,227 --> 00:07:10,230
これがトレーニングの全体像です

95
00:07:11,131 --> 00:07:15,669
続いて 冒頭で触れた
注意点についてです

96
00:07:16,737 --> 00:07:17,538
まず―

97
00:07:18,639 --> 00:07:22,943
分類器は必要な時だけ
実行しましょう

98
00:07:24,344 --> 00:07:28,649
畳み込みネットワークを
使っているからです

99
00:07:28,749 --> 00:07:35,289
つまり 実行すると
CPUやGPUも動きだすので

100
00:07:35,389 --> 00:07:38,292
必要な時だけにしましょう

101
00:07:38,892 --> 00:07:42,062
後ほどデモをお見せしますが

102
00:07:42,663 --> 00:07:48,602
カメラを動かしている時などは
分類はしないように

103
00:07:50,037 --> 00:07:55,275
静止した状態になったら
分類を実行するのです

104
00:07:56,009 --> 00:07:59,980
それには
レジストレーション機能を使って

105
00:08:00,080 --> 00:08:02,382
２つの画像を比較します

106
00:08:02,516 --> 00:08:09,189
どれくらいピクセルを変換すれば
画像が一致するかを判断するのです

107
00:08:09,523 --> 00:08:12,059
優秀なアルゴリズムで

108
00:08:12,159 --> 00:08:17,264
対象物が正確に撮れる状態か
教えてくれます

109
00:08:19,633 --> 00:08:23,871
VNTranslationalImage
RegistrationRequestを使うと

110
00:08:25,038 --> 00:08:27,975
必要な情報が得られます

111
00:08:28,075 --> 00:08:31,311
実際に動画で見てみましょう

112
00:08:31,445 --> 00:08:35,048
動画上に現れる黄色のラインは

113
00:08:35,148 --> 00:08:39,318
カメラや
レジストレーションリクエストが

114
00:08:39,419 --> 00:08:41,688
どう動いたかを表しています

115
00:08:41,855 --> 00:08:46,093
カメラを動かすと
ラインは長くなり

116
00:08:46,193 --> 00:08:49,229
動きを止めると短くなります

117
00:08:50,931 --> 00:08:55,903
カメラの動きによって
ラインの長さが変化します

118
00:08:56,904 --> 00:09:01,408
静止したタイミングで
分類器を実行します

119
00:08:56,904 --> 00:09:01,408
静止したタイミングで
分類器を実行します

120
00:09:03,677 --> 00:09:07,347
次の注意点は
代替策を用意すること

121
00:09:07,648 --> 00:09:09,349
大切ですよね

122
00:09:09,683 --> 00:09:12,085
分類を誤る場合もあります

123
00:09:13,554 --> 00:09:17,424
たとえ精度が高い分類器でも

124
00:09:17,524 --> 00:09:21,261
万が一の場合の
対応策は必要です

125
00:09:22,462 --> 00:09:25,799
手元に対象物がない場合も

126
00:09:25,899 --> 00:09:29,403
対応できるようにするには？

127
00:09:29,603 --> 00:09:33,474
Visionフレームワークの
バーコード検出を用いて

128
00:09:33,574 --> 00:09:36,543
データを読み取るのです

129
00:09:37,211 --> 00:09:40,681
では ここで
デモをお見せしましょう

130
00:09:41,748 --> 00:09:46,520
(拍手)

131
00:09:52,993 --> 00:09:57,297
スクリーンの右側に
見えるのがデバイスです

132
00:09:57,531 --> 00:10:01,201
ロボットの店の
アプリケーションを起動します

133
00:09:57,531 --> 00:10:01,201
ロボットの店の
アプリケーションを起動します

134
00:10:01,568 --> 00:10:04,638
動かしている間は
何も起きません

135
00:10:04,738 --> 00:10:09,009
対象物を表示して静止すると
ラインが現れて

136
00:10:09,109 --> 00:10:11,778
ステッピングモーターだと
判別します

137
00:10:12,112 --> 00:10:14,815
他にも試してみましょう

138
00:10:18,418 --> 00:10:20,287
これはマイクロコントローラ

139
00:10:23,357 --> 00:10:25,092
ステッピングモータードライバ

140
00:10:25,192 --> 00:10:28,128
手のひらに載せて…

141
00:10:29,663 --> 00:10:31,598
クローズドループベルトです

142
00:10:35,335 --> 00:10:37,504
これは親ねじです

143
00:10:37,604 --> 00:10:40,874
QRコードも認識します

144
00:10:41,408 --> 00:10:43,443
ケーブルが短いですね

145
00:10:46,046 --> 00:10:48,482
学習講座のページが開きました

146
00:10:48,582 --> 00:10:49,583
これで…

147
00:10:49,883 --> 00:10:50,584
フランク

148
00:10:51,418 --> 00:10:52,653
何でしょう

149
00:10:53,453 --> 00:10:54,188
フランク

150
00:10:55,622 --> 00:10:59,993
この部品でも
試してみてくれないか？

151
00:11:00,093 --> 00:11:01,895
上司のブレットです

152
00:11:04,331 --> 00:11:05,365
よろしく

153
00:11:08,502 --> 00:11:11,572
依頼はいつも ぎりぎりです

154
00:11:12,272 --> 00:11:14,374
また お願いするかも

155
00:11:15,275 --> 00:11:17,711
土曜は働きませんよ

156
00:11:18,212 --> 00:11:21,315
これはサーボモータですね

157
00:11:21,982 --> 00:11:24,351
では やってみましょうか

158
00:11:25,586 --> 00:11:26,987
試してみます

159
00:11:28,088 --> 00:11:30,524
認識できるでしょうか

160
00:11:30,624 --> 00:11:33,460
どうやら駄目なようです

161
00:11:33,560 --> 00:11:35,596
分類を誤りました

162
00:11:35,863 --> 00:11:37,030
バグですね

163
00:11:37,865 --> 00:11:39,399
修正が必要です

164
00:11:39,867 --> 00:11:41,201
直したい人は？

165
00:11:41,668 --> 00:11:44,338
(拍手)

166
00:11:44,438 --> 00:11:46,340
立候補者は？

167
00:11:46,473 --> 00:11:47,541
分かりましたよ

168
00:11:49,409 --> 00:11:54,915
ではまず サーボモータの
画像を用意します

169
00:11:55,015 --> 00:11:59,586
スタジオへ行って
撮影してもいいのですが

170
00:11:59,820 --> 00:12:04,458
今回は 私が愛用している
カメラを使いましょう

171
00:11:59,820 --> 00:12:04,458
今回は 私が愛用している
カメラを使いましょう

172
00:12:05,259 --> 00:12:09,930
サーボモータの画像を
たくさん撮っていきます

173
00:12:12,366 --> 00:12:17,004
角度を変えつつ 他の物が
写らないようにします

174
00:12:23,410 --> 00:12:27,347
画像は最低でも10枚
用意しましょう

175
00:12:27,447 --> 00:12:31,018
背景を変えて撮ることも
大切です

176
00:12:35,222 --> 00:12:40,260
この上で数枚撮ったあとは
手のひらに載せて撮ります

177
00:12:46,833 --> 00:12:49,736
これで画像数は十分です

178
00:12:51,305 --> 00:12:55,642
では実際に
トレーニングを行いましょう

179
00:12:57,211 --> 00:12:58,111
よし

180
00:12:59,613 --> 00:13:04,251
撮った画像を
アプリケーションで取り込みます

181
00:12:59,613 --> 00:13:04,251
撮った画像を
アプリケーションで取り込みます

182
00:13:04,985 --> 00:13:06,520
QuickTime Playerを隠して…

183
00:13:09,122 --> 00:13:12,726
Finder内に
表示されているのは

184
00:13:12,826 --> 00:13:17,264
モデルのトレーニングに使った
データ一式です

185
00:13:17,731 --> 00:13:19,833
新しいフォルダを作成し―

186
00:13:21,235 --> 00:13:22,970
“Servo”と名付けます

187
00:13:25,772 --> 00:13:30,911
撮影したばかりの画像を
全て選択したら…

188
00:13:34,515 --> 00:13:37,784
“Servo”フォルダにドラッグ

189
00:13:41,121 --> 00:13:44,458
これで準備ができました

190
00:13:44,558 --> 00:13:46,693
上司のせいで―

191
00:13:46,894 --> 00:13:50,330
またトレーニングしなければ
なりません

192
00:13:50,497 --> 00:13:51,298
よし

193
00:13:52,232 --> 00:13:57,571
ここでは単純なスクリプトの
Playgroundを用います

194
00:13:57,671 --> 00:14:01,808
あとでアプリケーションに
組み込むためです

195
00:13:57,671 --> 00:14:01,808
あとでアプリケーションに
組み込むためです

196
00:14:02,342 --> 00:14:06,246
フォルダに入れたデータを
ポイントして

197
00:14:06,713 --> 00:14:11,485
分類器をトレーニングしたら
モデルを書き出します

198
00:14:12,686 --> 00:14:17,057
ご覧のように
次々と処理が行われます

199
00:14:17,357 --> 00:14:21,929
フォルダに保存した画像が
全て読み込まれ―

200
00:14:22,229 --> 00:14:25,466
データが解析されていきます

201
00:14:25,566 --> 00:14:32,239
必要に応じてサイズが縮小され
モデルの学習が進められます

202
00:14:32,339 --> 00:14:36,677
わずかなコードで
複雑な処理が行われ

203
00:14:36,777 --> 00:14:41,281
作業が完了すると
モデルの出来上がりです

204
00:14:41,715 --> 00:14:43,550
終わりそうですね

205
00:14:44,818 --> 00:14:46,153
もう少しです

206
00:14:47,120 --> 00:14:49,089
完成しました

207
00:14:53,093 --> 00:14:59,266
先ほど 店のアプリケーションで
使っていたのと同じモデルです

208
00:15:00,033 --> 00:15:04,171
サイズは わずか148KBしかなく

209
00:15:04,438 --> 00:15:08,008
スタートアップ画面より
小容量です

210
00:15:09,042 --> 00:15:15,749
(拍手)

211
00:15:15,849 --> 00:15:19,820
ここで強調しておきたいことが
あります

212
00:15:21,321 --> 00:15:26,593
必要な画像はカラーで
サイズは299×299ピクセル

213
00:15:26,693 --> 00:15:30,197
分類器の場合
多くが この大きさです

214
00:15:31,331 --> 00:15:35,836
それでは モデルが
完成したようなので

215
00:15:35,936 --> 00:15:40,974
plist形式の
商品データベースを開いて

216
00:15:42,109 --> 00:15:44,778
サーボモータの情報を追加します

217
00:15:48,048 --> 00:15:51,051
これを
“Servo”と名付けましょう

218
00:15:52,686 --> 00:15:56,223
そしてラベルを入力します

219
00:15:56,323 --> 00:15:59,459
“サーボモータ”と

220
00:16:00,861 --> 00:16:02,896
説明は―

221
00:16:03,697 --> 00:16:08,735
“サッサッと動くモータ”

222
00:16:09,770 --> 00:16:10,904
こんな感じ

223
00:16:12,139 --> 00:16:13,907
技術的ですね

224
00:16:14,775 --> 00:16:16,143
うまくいくでしょうか

225
00:16:17,177 --> 00:16:18,946
アプリケーションを起動

226
00:16:28,355 --> 00:16:30,691
では 試してみましょう

227
00:16:32,759 --> 00:16:34,328
正しく認識しました

228
00:16:34,428 --> 00:16:41,602
(拍手)

229
00:16:41,702 --> 00:16:48,208
世界で初めてステージ上で
分類器をトレーニングし

230
00:16:48,308 --> 00:16:50,077
実行しました

231
00:16:50,711 --> 00:16:52,379
少し緊張しました

232
00:16:52,513 --> 00:16:53,981
(笑い声)

233
00:16:54,081 --> 00:16:55,516
(拍手)

234
00:16:55,616 --> 00:16:56,283
ありがとう

235
00:16:56,383 --> 00:17:01,121
(拍手)

236
00:16:56,383 --> 00:17:01,121
(拍手)

237
00:17:01,788 --> 00:17:07,194
今の工程について
少し説明させてください

238
00:17:08,929 --> 00:17:11,131
コードをお見せします

239
00:17:12,266 --> 00:17:13,066
よし

240
00:17:14,201 --> 00:17:17,137
不要なものは閉じますね

241
00:17:18,771 --> 00:17:20,040
少し広げて…

242
00:17:22,108 --> 00:17:25,244
今の流れを解説します

243
00:17:25,345 --> 00:17:28,882
まず
sequenceRequestHandlerを作り

244
00:17:28,982 --> 00:17:31,952
レジストレーションに用います

245
00:17:32,052 --> 00:17:37,024
すでに話があったとおり
物体の追跡に適しています

246
00:17:37,958 --> 00:17:41,528
リクエストを作成したら
順に並べます

247
00:17:41,628 --> 00:17:46,667
レジストレーションには
結果が15件出るようにして

248
00:17:46,767 --> 00:17:50,404
カメラが静止しているか
確認します

249
00:17:51,238 --> 00:17:55,242
その間 保持するバッファは１つ

250
00:17:55,342 --> 00:17:57,611
分類を行う間もです

251
00:17:58,679 --> 00:18:03,383
時間がかかるので
別のキューに入れます

252
00:17:58,679 --> 00:18:03,383
時間がかかるので
別のキューに入れます

253
00:18:05,919 --> 00:18:11,291
ご覧いただいているのが
私が使用したコードです

254
00:18:11,458 --> 00:18:14,795
Visionタスクの設定方法は？

255
00:18:15,095 --> 00:18:18,532
実行するタスクは
バーコードリクエストと

256
00:18:18,632 --> 00:18:20,968
分類リクエストです

257
00:18:21,268 --> 00:18:23,203
まず
バーコードリクエストです

258
00:18:23,904 --> 00:18:28,342
完了ハンドラの状態を
確認しましょう

259
00:18:28,909 --> 00:18:34,314
バーコードは１つなので
最初の結果だけを見ます

260
00:18:35,015 --> 00:18:38,585
デコードできるかも
確かめましょう

261
00:18:38,685 --> 00:18:42,656
デモでは うまくいきましたね

262
00:18:43,957 --> 00:18:47,327
これをリクエストに加えます

263
00:18:47,427 --> 00:18:49,797
続いて分類リクエストです

264
00:18:49,897 --> 00:18:53,233
今回使ったのは私の分類器で

265
00:18:53,333 --> 00:18:59,239
それをバンドルからロードし
モデルを作りました

266
00:18:59,339 --> 00:19:02,910
今回はコード補完を
使っていません

267
00:18:59,339 --> 00:19:02,910
今回はコード補完を
使っていません

268
00:19:03,010 --> 00:19:06,613
これは私がCore MLで使っている
唯一のコードですし

269
00:19:06,713 --> 00:19:09,516
自分で調整できるからです

270
00:19:09,616 --> 00:19:14,254
コード補完を使うことも
効果的だと思います

271
00:19:14,788 --> 00:19:20,661
こうしてVision Core MLモデルと
リクエストを構築しました

272
00:19:20,761 --> 00:19:26,633
リクエストが返されると
完了ハンドラが実行されます

273
00:19:26,733 --> 00:19:31,205
どんな分類結果が
出てきたか確認して

274
00:19:31,738 --> 00:19:34,007
しきい値を設定します

275
00:19:34,174 --> 00:19:38,846
ここでは実験的に
0.98と設定しました

276
00:19:38,946 --> 00:19:42,349
正解の自信が
98％あるという意味です

277
00:19:43,383 --> 00:19:49,256
この設定により
不確実な分類結果を除去できます

278
00:19:49,356 --> 00:19:51,525
あとで説明します

279
00:19:52,893 --> 00:19:54,928
リクエストがそろいました

280
00:19:55,495 --> 00:19:59,466
これらを実行する際に
関数を設定して

281
00:19:59,566 --> 00:20:02,269
対象の画像を解析させます

282
00:19:59,566 --> 00:20:02,269
対象の画像を解析させます

283
00:20:02,436 --> 00:20:05,239
実際に解析を行う時には

284
00:20:05,572 --> 00:20:11,245
device orientationで
デバイスの向きを確認します

285
00:20:11,879 --> 00:20:17,017
VNImageRequestHandlerを
実行するバッファに作ります

286
00:20:17,851 --> 00:20:22,256
そして非同期的に実行させます

287
00:20:24,291 --> 00:20:29,763
以上がCore MLと
バーコードの処理方法です

288
00:20:30,264 --> 00:20:34,535
次は 対象物を
正確に捉える仕組みです

289
00:20:34,635 --> 00:20:39,339
リセット可能なキューに
ポイントを追加しました

290
00:20:39,940 --> 00:20:43,610
そして
記録したポイントのキューを

291
00:20:43,710 --> 00:20:46,346
調べる機能を作りました

292
00:20:46,447 --> 00:20:49,383
自分の経験から 最大でも―

293
00:20:49,483 --> 00:20:53,687
20ピクセル程度になるよう
設定しました

294
00:20:53,787 --> 00:20:58,392
すると カメラが静止したと
認識されます

295
00:20:59,827 --> 00:21:01,728
次は出力のキャプチャです

296
00:20:59,827 --> 00:21:01,728
次は出力のキャプチャです

297
00:21:01,828 --> 00:21:06,233
これはカメラのバッファと
AVFoundationのコードです

298
00:21:06,700 --> 00:21:10,104
前のピクセルバッファを
残しておき

299
00:21:10,204 --> 00:21:13,740
レジストレーションと比較し

300
00:21:14,007 --> 00:21:17,311
終了したらスワップアウトします

301
00:21:17,844 --> 00:21:21,915
VNTranslationalImage
RegistrationRequestを作成し

302
00:21:22,649 --> 00:21:26,820
sequenceRequestHandler上で
リクエストを実行します

303
00:21:27,287 --> 00:21:31,158
結果が出たら内容を確認します

304
00:21:31,658 --> 00:21:33,527
配列に追加して

305
00:21:34,428 --> 00:21:38,098
カメラが静止した状態か
確認します

306
00:21:38,499 --> 00:21:43,570
確認できたら黄色いボックスを
表示するようにします

307
00:21:44,404 --> 00:21:47,374
バッファは解析済みなので

308
00:21:47,708 --> 00:21:52,646
画像を解析するように
指示を出します

309
00:21:54,548 --> 00:21:59,686
ご覧のとおり
非同期呼び出しの最後に

310
00:21:59,786 --> 00:22:02,656
バッファを解放します

311
00:21:59,786 --> 00:22:02,656
バッファを解放します

312
00:22:02,856 --> 00:22:06,693
そして 他にバッファが
ないか確認して

313
00:22:06,827 --> 00:22:12,466
１つのバッファだけが
動いていることを確かめます

314
00:22:12,566 --> 00:22:16,470
カメラのフレームを
安定させるためです

315
00:22:17,371 --> 00:22:22,209
実行に際して
幾つか補足させてください

316
00:22:22,309 --> 00:22:23,410
１つ目は―

317
00:22:24,478 --> 00:22:29,783
画面の下方にある
コンソールを見ると実際に…

318
00:22:29,883 --> 00:22:33,120
少しお待ちください

319
00:22:35,923 --> 00:22:37,758
画面が現れます

320
00:22:38,325 --> 00:22:40,661
信頼スコアが低いですね

321
00:22:40,761 --> 00:22:45,199
ただの白い背景で
何も認識できていないのです

322
00:22:45,499 --> 00:22:48,936
識別可能な物を表示すると
すぐに…

323
00:22:49,570 --> 00:22:52,005
信頼スコアが上がります

324
00:22:52,105 --> 00:22:55,709
対象物を認識できたからです

325
00:22:56,844 --> 00:22:59,213
もう１つ ご覧ください

326
00:23:01,849 --> 00:23:05,119
CPUに注目してみましょう

327
00:23:07,054 --> 00:23:11,792
今はただ 画像を
表示しているだけですが

328
00:23:12,159 --> 00:23:14,828
カメラを動かしてみます

329
00:23:15,662 --> 00:23:19,800
CPU使用率は22％に
上がりました

330
00:23:19,900 --> 00:23:24,204
分類器を実行すると
さらに上がるので

331
00:23:24,671 --> 00:23:28,876
必要時のみ動かすことを
お勧めします

332
00:23:31,011 --> 00:23:33,981
盛りだくさんでしたね

333
00:23:34,081 --> 00:23:38,719
スライドに戻って
おさらいをしましょう

334
00:23:39,786 --> 00:23:41,588
スライドでしたね

335
00:23:41,688 --> 00:23:47,094
(拍手)

336
00:23:48,161 --> 00:23:49,463
おさらいです

337
00:23:49,930 --> 00:23:52,900
まずは対象物の認識

338
00:23:53,867 --> 00:23:55,569
VNSequenceRequestHandlerと

339
00:23:55,669 --> 00:23:59,039
VNTranslationalImage
RegistrationRequestで

340
00:24:00,140 --> 00:24:02,309
前のフレームと比較します

341
00:24:03,377 --> 00:24:08,348
すると アライメントの
変換が分かるので

342
00:24:08,448 --> 00:24:14,354
フレームが どう移行したか
知ることができます

343
00:24:15,923 --> 00:24:19,660
カメラが静止した時だけ
解析するには

344
00:24:20,194 --> 00:24:25,332
VNImageRequestHandlerを作り

345
00:24:25,966 --> 00:24:31,839
バーコード検出と分類機能を
実行させます

346
00:24:32,105 --> 00:24:36,577
そうすると
Visionが最適化され

347
00:24:36,677 --> 00:24:40,581
より速い処理が可能になります

348
00:24:43,083 --> 00:24:47,488
バッファをいくつ保持するかも
話しましたね

349
00:24:47,588 --> 00:24:49,923
自分で管理しましょう

350
00:24:50,891 --> 00:24:55,629
畳み込みネットワークなどは
時間がかかるので

351
00:24:55,929 --> 00:25:00,200
バックグラウンドキューで
動かすようにします

352
00:24:55,929 --> 00:25:00,200
バックグラウンドキューで
動かすようにします

353
00:25:00,300 --> 00:25:04,638
そうすれば
同時にカメラを使えます

354
00:25:04,972 --> 00:25:10,077
撮影時 キューにタスクが
たまることは望まないので

355
00:25:10,177 --> 00:25:14,348
今回はバッファを
１つだけにしましたね

356
00:25:14,448 --> 00:25:17,084
すると うまくいきました

357
00:25:17,184 --> 00:25:21,955
必要なバッファだけが
動いていることを確認し

358
00:25:22,122 --> 00:25:28,195
終了したらリセットして
新たなバッファを使います

359
00:25:31,098 --> 00:25:36,970
なぜ Core MLモデルに
Visionを使うのでしょうか

360
00:25:38,105 --> 00:25:42,042
それには重要な理由があります

361
00:25:42,209 --> 00:25:45,445
モデルを思い出してみましょう

362
00:25:45,546 --> 00:25:49,416
299×299ピクセルという
妙なサイズの画像で―

363
00:25:49,516 --> 00:25:53,487
このモデルは
トレーニングをします

364
00:25:53,787 --> 00:25:58,959
でも 撮影される画像のサイズは
さまざまですね

365
00:25:59,760 --> 00:26:05,132
そこで Visionが
その画像を処理して

366
00:25:59,760 --> 00:26:05,132
そこで Visionが
その画像を処理して

367
00:26:05,232 --> 00:26:09,136
RGB変換や
サイズ縮小をしてくれます

368
00:26:09,403 --> 00:26:14,741
Visionの働きにより
画像処理が容易になるのです

369
00:26:17,377 --> 00:26:21,949
続いて 物体認識について
説明しましょう

370
00:26:24,451 --> 00:26:28,722
なお クロワッサンが
危険な目に遭いますので

371
00:26:28,822 --> 00:26:31,258
気が弱い方はご注意ください

372
00:26:34,528 --> 00:26:37,898
物体認識に用いるのは

373
00:26:37,998 --> 00:26:42,903
YOLOという技術に
基づいたモデルです

374
00:26:43,003 --> 00:26:46,206
極めて速度が速いモデルで

375
00:26:46,306 --> 00:26:50,711
境界ボックスや
ラベルを取得できます

376
00:26:50,811 --> 00:26:54,181
さらに複数の対象物を
検出します

377
00:26:57,417 --> 00:27:00,954
位置が分かることも
利点ですが

378
00:26:57,417 --> 00:27:00,954
位置が分かることも
利点ですが

379
00:27:01,054 --> 00:27:06,326
総合的な分類器ほど
分類が得意ではありません

380
00:27:07,327 --> 00:27:10,597
画像認識より多く
トレーニングが必要です

381
00:27:10,697 --> 00:27:15,769
昨日のセッションでは
Turi Createを用いた―

382
00:27:15,869 --> 00:27:20,007
トレーニング方法の
実演がありましたね

383
00:27:21,708 --> 00:27:24,912
では デモをお見せしましょう

384
00:27:30,551 --> 00:27:32,553
ロボットの店は閉店

385
00:27:35,422 --> 00:27:36,690
朝食の時間です

386
00:27:42,196 --> 00:27:43,130
では―

387
00:27:43,897 --> 00:27:49,703
Breakfast Finderという
アプリケーションを起動します

388
00:27:52,206 --> 00:27:55,008
クロワッサンが
認識されていますね

389
00:27:55,209 --> 00:27:58,645
それにベーグルとバナナも

390
00:28:00,147 --> 00:28:03,817
フレーム内の全てを
認識できています

391
00:28:04,785 --> 00:28:08,989
料理番組の場合
作り方を見せながらも

392
00:28:09,089 --> 00:28:11,658
実は事前に
完成版が出来ています

393
00:28:12,459 --> 00:28:14,361
でも このクロワッサンは

394
00:28:14,461 --> 00:28:19,299
モデルよりも
ずっと新しいはずです

395
00:28:20,067 --> 00:28:21,568
(笑い声)

396
00:28:21,668 --> 00:28:22,703
確かに

397
00:28:24,104 --> 00:28:25,572
クロワッサンです

398
00:28:25,973 --> 00:28:31,578
(拍手)

399
00:28:34,314 --> 00:28:35,082
さて…

400
00:28:35,349 --> 00:28:37,017
(笑い声)

401
00:28:38,385 --> 00:28:40,654
なかなか飲み込めません

402
00:28:41,188 --> 00:28:42,256
(笑い声)

403
00:28:44,491 --> 00:28:47,261
コードを確認しましょう

404
00:28:53,133 --> 00:28:55,035
相違点はどこでしょう？

405
00:28:56,069 --> 00:28:59,206
リクエストの設定方法です

406
00:28:59,706 --> 00:29:05,746
先ほどと同じように
Core MLモデルを使います

407
00:28:59,706 --> 00:29:05,746
先ほどと同じように
Core MLモデルを使います

408
00:29:05,846 --> 00:29:07,848
リクエストを作って

409
00:29:08,382 --> 00:29:12,319
結果をどう出力するか決めます

410
00:29:17,357 --> 00:29:21,395
これを容易にする方法があります

411
00:29:21,762 --> 00:29:26,366
ここに 新しい
オブジェクトである―

412
00:29:26,467 --> 00:29:31,405
VNRecognizedObjectObservationが
ありますね

413
00:29:31,505 --> 00:29:37,077
これで 境界ボックスや
ラベルが得られます

414
00:29:37,911 --> 00:29:40,147
ここで１つ お見せします

415
00:29:40,914 --> 00:29:43,817
アプリケーションを
起動しましょう

416
00:29:51,325 --> 00:29:52,292
よし

417
00:29:53,927 --> 00:29:56,063
ブレークポイントで停止しました

418
00:29:56,163 --> 00:30:01,802
では 結果が１つに
絞られる経緯を説明します

419
00:29:56,163 --> 00:30:01,802
では 結果が１つに
絞られる経緯を説明します

420
00:30:04,471 --> 00:30:05,772
この部分を使って…

421
00:30:11,478 --> 00:30:17,651
“objectObservation.labels”と
入力します

422
00:30:19,486 --> 00:30:23,190
ご覧のとおり
結果は複数出ます

423
00:30:23,290 --> 00:30:25,359
ベーグルにバナナ

424
00:30:25,459 --> 00:30:28,028
コーヒーはありませんが

425
00:30:28,128 --> 00:30:30,731
クロワッサンや卵など

426
00:30:30,831 --> 00:30:34,835
確率の高い順番に
分類されています

427
00:30:34,935 --> 00:30:40,107
だから １番目を読み取る
設定にしているのです

428
00:30:40,207 --> 00:30:44,444
分類結果には
登録した全ての物の名称が

429
00:30:44,711 --> 00:30:47,347
出るようになっています

430
00:30:48,549 --> 00:30:54,555
Breakfast Finderを閉じて
再びスライドに戻りましょう

431
00:30:58,425 --> 00:31:02,196
この機能を可能にしたのが
新しいAPIの―

432
00:30:58,425 --> 00:31:02,196
この機能を可能にしたのが
新しいAPIの―

433
00:31:02,296 --> 00:31:05,399
VNRecognizedObjectObservation
です

434
00:31:07,201 --> 00:31:11,371
Core MLモデルの
リクエストが実行され

435
00:31:11,472 --> 00:31:17,411
かつ 物体認識を行う場合
自動的に出てきます

436
00:31:20,881 --> 00:31:23,750
YOLOを基にしています

437
00:31:23,851 --> 00:31:29,056
YOLOは目新しくないと
思うかもしれませんが

438
00:31:29,790 --> 00:31:34,461
今までは多くのコードが
必要でしたね

439
00:31:34,561 --> 00:31:38,899
これなら たった数行で済むので

440
00:31:38,999 --> 00:31:41,702
YOLOモデルを簡単に使えます

441
00:31:42,069 --> 00:31:44,071
コードに戻りましょう

442
00:31:44,204 --> 00:31:48,375
モデルを作成したら
リクエストを作ります

443
00:31:48,876 --> 00:31:50,844
完了ハンドラは

444
00:31:50,978 --> 00:31:55,682
複数の対象物の領域を確認します

445
00:31:55,916 --> 00:32:00,420
さらに 境界ボックスや
ラベルを提示してくれます

446
00:31:55,916 --> 00:32:00,420
さらに 境界ボックスや
ラベルを提示してくれます

447
00:32:04,057 --> 00:32:07,161
１つ補足したいことがあります

448
00:32:08,328 --> 00:32:13,934
フレームごとに検出を実行したので
画面上で四角形が揺れていました

449
00:32:14,902 --> 00:32:18,005
トラッキングを
用いたほうがよいでしょう

450
00:32:18,605 --> 00:32:23,510
モデルを通常より速く
実行させられるからです

451
00:32:25,145 --> 00:32:29,850
トラッキングリクエストは
時間がかかりません

452
00:32:31,185 --> 00:32:35,889
画面上で対象物を追う際に
トラッキングをすると

453
00:32:37,024 --> 00:32:39,660
動作が速くなります

454
00:32:39,793 --> 00:32:42,963
しかもスムージングが
可能なので

455
00:32:43,063 --> 00:32:45,866
四角形の揺れも収まります

456
00:32:45,966 --> 00:32:50,270
実際に動きが
滑らかになった実例もあります

457
00:32:50,838 --> 00:32:52,873
トラッキングについては

458
00:32:54,041 --> 00:32:59,947
前のセッションで実装などの
詳しい話がありました

459
00:33:01,949 --> 00:33:02,883
最後に―

460
00:33:03,250 --> 00:33:07,988
Visionに関して
理解を深めていきましょう

461
00:33:08,822 --> 00:33:12,192
Visionフレームワークの
注意点があります

462
00:33:12,860 --> 00:33:18,432
まず よく問題となるのが
画像の向きです

463
00:33:19,900 --> 00:33:23,871
幾つかのVisionや
新しい顔検出機能には

464
00:33:23,971 --> 00:33:28,842
画像の向きに
依存しないものもありますが

465
00:33:29,042 --> 00:33:30,978
以前は違いました

466
00:33:32,045 --> 00:33:35,716
正しい向きを
把握する必要があります

467
00:33:36,283 --> 00:33:41,989
しかも 自分が見た向きと
ディスクに保存された向きが

468
00:33:42,089 --> 00:33:44,291
違う場合もあるのです

469
00:33:44,925 --> 00:33:50,197
EXIF orientationがあれば
向きが特定できます

470
00:33:51,131 --> 00:33:57,971
EXIFは撮影時に
正しい方向を把握します

471
00:33:58,071 --> 00:34:02,443
その情報がVisionに送られると

472
00:33:58,071 --> 00:34:02,443
その情報がVisionに送られると

473
00:34:02,576 --> 00:34:08,114
Visionが情報を取得し
調整してくれるのです

474
00:34:09,149 --> 00:34:13,587
しかし 先ほどのデモのように
動画の場合は

475
00:34:13,687 --> 00:34:16,089
自分で情報を送ります

476
00:34:16,188 --> 00:34:21,428
そこで UIDevice.current.
orientationを確認し

477
00:34:21,527 --> 00:34:27,167
CGImagePropertyOrientationに
変換します

478
00:34:29,369 --> 00:34:32,306
次は座標系についてです

479
00:34:33,373 --> 00:34:36,543
Visionの場合
原点は左下隅です

480
00:34:37,710 --> 00:34:43,217
そして画像に沿って
垂直に処理されるため

481
00:34:43,317 --> 00:34:45,418
向きが重要なのです

482
00:34:47,187 --> 00:34:50,591
正規化された座標上で
処理されますが

483
00:34:50,891 --> 00:34:54,527
ピクセル数が不明な
レジストレーションは別です

484
00:34:54,761 --> 00:35:00,801
正規座標系とは 座標が0.0から
1.1までの範囲のものです

485
00:34:54,761 --> 00:35:00,801
正規座標系とは 座標が0.0から
1.1までの範囲のものです

486
00:35:02,302 --> 00:35:07,174
画像上で顔とランドマークが
検出されています

487
00:35:07,441 --> 00:35:10,110
境界ボックスが表示され

488
00:35:10,210 --> 00:35:15,716
その中にランドマークと
その座標が示されています

489
00:35:17,184 --> 00:35:21,789
座標空間に戻る際は関数と
VNUtils.hを用います

490
00:35:21,889 --> 00:35:27,694
VNImageRectForNormalizedRectが
変換するのと同じです

491
00:35:31,298 --> 00:35:36,970
続いて 先ほども登場した
信頼スコアについて話します

492
00:35:38,705 --> 00:35:43,110
アルゴリズムによる
分類結果とその信頼度は

493
00:35:43,844 --> 00:35:50,284
結果を分析する上で
とても重要な要素となります

494
00:35:50,384 --> 00:35:54,588
０は信頼度が低く
１では高くなります

495
00:35:58,892 --> 00:35:59,660
クリック

496
00:36:02,262 --> 00:36:03,263
出ました

497
00:36:03,364 --> 00:36:06,366
残念ながら
アルゴリズムによって

498
00:36:06,467 --> 00:36:10,337
信頼スコアの基準は
異なります

499
00:36:10,437 --> 00:36:16,477
例えば テキスト検出の場合
信頼スコアは いつも１です

500
00:36:16,610 --> 00:36:20,614
検出できなければ
何も実行されないからです

501
00:36:21,181 --> 00:36:27,287
しかし 分類器は
幅広いスコアを提示します

502
00:36:27,521 --> 00:36:28,889
例を見てみます

503
00:36:30,591 --> 00:36:35,662
最初は ロボットの店の
商品画像です

504
00:36:36,063 --> 00:36:37,931
モデルに分類させると

505
00:36:38,298 --> 00:36:42,369
“ステッピングモーター”と
高スコアで出ました

506
00:36:43,637 --> 00:36:45,539
次の画像では

507
00:36:45,806 --> 00:36:49,076
ギャラリーにあったモデルを
使います

508
00:36:49,710 --> 00:36:52,980
モデルの能力を
比べるわけではなく

509
00:36:53,080 --> 00:36:57,751
結果や信頼スコアを
分析するためです

510
00:36:58,485 --> 00:37:02,089
では 分類結果を
見てみましょう

511
00:36:58,485 --> 00:37:02,089
では 分類結果を
見てみましょう

512
00:37:03,390 --> 00:37:07,961
悪くない結果ですが
自信は なさそうですね

513
00:37:08,062 --> 00:37:11,899
信頼スコアは0.395と低めです

514
00:37:11,999 --> 00:37:15,202
でも“砂”や“浜辺”は
合っていますよね

515
00:37:15,436 --> 00:37:19,606
画像を探す時には
使えると思いますが

516
00:37:19,707 --> 00:37:23,043
ラベルにできるかは
疑問が残ります

517
00:37:24,244 --> 00:37:25,846
続いてはこちら

518
00:37:26,980 --> 00:37:30,384
スクーターと女性が
写っています

519
00:37:31,752 --> 00:37:34,621
“サツマイモ”とは
心外でしょうね

520
00:37:34,721 --> 00:37:36,356
(笑い声)

521
00:37:39,393 --> 00:37:41,228
さらに もう１つ

522
00:37:41,562 --> 00:37:43,397
コードの画像です

523
00:37:44,097 --> 00:37:45,966
分類結果は？

524
00:37:47,267 --> 00:37:50,270
愚かなことに
“ウェブサイト”だそうです

525
00:37:53,507 --> 00:37:56,510
信頼スコアについて
まとめます

526
00:37:58,245 --> 00:38:02,683
スコアが1.0でも
100％正解ではありません

527
00:37:58,245 --> 00:38:02,683
スコアが1.0でも
100％正解ではありません

528
00:38:02,783 --> 00:38:05,152
アルゴリズムには
沿っていますが

529
00:38:05,252 --> 00:38:10,057
先ほどの例のように
人間の認識と差があります

530
00:38:11,391 --> 00:38:16,730
よって アプリケーションに
使う時はご注意ください

531
00:38:16,830 --> 00:38:20,701
例えば 医療用の
アプリケーションなら

532
00:38:21,168 --> 00:38:25,172
結果の精度によって
病気の告知方法に

533
00:38:25,272 --> 00:38:28,075
配慮が必要でしょう

534
00:38:30,577 --> 00:38:33,313
ここで使える技が２つあります

535
00:38:33,414 --> 00:38:39,286
デモのように 信頼スコアに
しきい値を用いる方法が１つ

536
00:38:39,386 --> 00:38:43,223
信頼度の低い結果が
除去されます

537
00:38:43,590 --> 00:38:49,296
画像検索アプリケーションの中で
使う方法が２つ目です

538
00:38:49,396 --> 00:38:51,565
スコアが低くても

539
00:38:51,665 --> 00:38:56,103
正解が含まれる場合が
あるからです

540
00:39:01,074 --> 00:39:04,745
さらに詳しい情報は
デベロッパWebサイトでどうぞ

541
00:39:04,845 --> 00:39:08,515
明日15時から
ラボも行いますので

542
00:39:08,615 --> 00:39:10,851
ぜひ ご参加ください

543
00:39:11,585 --> 00:39:16,924
アプリケーションデベロッパの
皆さんに感謝します

544
00:39:17,024 --> 00:39:19,793
今後も楽しみにしています

545
00:39:19,893 --> 00:39:23,397
ご来場ありがとうございました

546
00:39:23,497 --> 00:39:27,801
(拍手)