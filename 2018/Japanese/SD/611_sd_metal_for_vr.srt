
1
00:00:07,074 --> 00:00:16,283
(音楽)

2
00:00:17,050 --> 00:00:22,689
(拍手)

3
00:00:23,790 --> 00:00:24,892
こんにちは

4
00:00:24,992 --> 00:00:29,997
Architectureチームの
カロル･ガシンスキーです

5
00:00:31,198 --> 00:00:34,201
今日はまず VRの観点から

6
00:00:34,301 --> 00:00:38,572
macOS Mojaveの
新機能を簡単に説明します

7
00:00:38,939 --> 00:00:41,241
次にMetal 2について

8
00:00:41,341 --> 00:00:45,812
特にVR向けの特徴を
詳しく説明します

9
00:00:46,680 --> 00:00:52,352
最後にVRアプリケーション開発の
高度な技を紹介します

10
00:00:53,820 --> 00:00:56,990
我々は最近 優れたGPUを
搭載した―

11
00:00:57,090 --> 00:00:59,226
iMacとiMac Proを
発表しました

12
00:00:59,526 --> 00:01:03,664
iMacは 今や
AMDのPolaris系GPUと

13
00:00:59,526 --> 00:01:03,664
iMacは 今や
AMDのPolaris系GPUと

14
00:01:03,764 --> 00:01:07,401
8GBのビデオメモリを
搭載しています

15
00:01:07,634 --> 00:01:13,106
一方 iMac Proは
更に高性能のGPUと

16
00:01:13,207 --> 00:01:15,976
最大16GBのビデオメモリを搭載

17
00:01:16,476 --> 00:01:19,980
これは かなりパワフルです

18
00:01:20,347 --> 00:01:23,851
しかも これだけではありません

19
00:01:24,518 --> 00:01:27,921
外付けGPUを利用すれば

20
00:01:28,088 --> 00:01:32,392
どのMacも高性能の
ワークステーションになります

21
00:01:33,260 --> 00:01:36,830
処理速度は
10テラフロップス以上です

22
00:01:37,064 --> 00:01:38,265
そして更に―

23
00:01:39,032 --> 00:01:44,872
HTC Viveヘッドマウントディスプレイの
プラグアンドプレイに対応します

24
00:01:45,205 --> 00:01:50,978
Vive Proは1440×1600の
２枚のパネルを備え

25
00:01:52,179 --> 00:01:55,182
画素密度は615ppiです

26
00:01:56,016 --> 00:01:59,319
旧製品に比べ
解像度は78パーセント向上

27
00:01:59,419 --> 00:02:04,024
画素密度は
37パーセント向上しました

28
00:01:59,419 --> 00:02:04,024
画素密度は
37パーセント向上しました

29
00:02:05,158 --> 00:02:08,362
パネルの性能向上に加え

30
00:02:08,461 --> 00:02:11,465
デュアルカメラを搭載しました

31
00:02:11,565 --> 00:02:17,938
このカメラを用いて Mac上で
パススルービデオの実験ができます

32
00:02:18,172 --> 00:02:22,309
最新のトラッキングシステムにも
対応しました

33
00:02:23,777 --> 00:02:28,582
ではVRアプリケーションを
どう開発すればよいか？

34
00:02:28,949 --> 00:02:32,052
HTC ViveとVive Proは
いずれも

35
00:02:32,152 --> 00:02:36,256
Valve社のSteamVRランタイムと
連動しており

36
00:02:36,456 --> 00:02:40,761
VR Compositorなど
多くのサービスが使えます

37
00:02:41,261 --> 00:02:44,865
オープンソースの
VRフレームワークもあり

38
00:02:44,965 --> 00:02:49,937
macOS上で
SteamVRを試せます

39
00:02:50,504 --> 00:02:54,308
Valve社やHTC社との
密接な連携で

40
00:02:54,408 --> 00:03:00,814
SteamVRのmacOS対応を
可能にしました

41
00:02:54,408 --> 00:03:00,814
SteamVRのmacOS対応を
可能にしました

42
00:03:02,883 --> 00:03:08,755
では Metalの新機能を用いて
masOS Mojaveで

43
00:03:08,856 --> 00:03:12,159
VRアプリケーションを
最適化する方法です

44
00:03:13,260 --> 00:03:16,530
アプリケーションと
VR Compositorの

45
00:03:16,630 --> 00:03:19,099
相互関係を見てみましょう

46
00:03:19,867 --> 00:03:23,937
まず 左右の画像が
レンダリングされ

47
00:03:24,037 --> 00:03:26,607
2Dマルチサンプルテクスチャと
なります

48
00:03:26,740 --> 00:03:31,645
そしてIOSurfaceテクスチャに
変換され

49
00:03:31,745 --> 00:03:34,915
VR Compositorに
渡されます

50
00:03:35,549 --> 00:03:40,187
VR Compositorが
最終処理として

51
00:03:40,287 --> 00:03:44,658
ゆがみ補正 色収差などの
操作を行います

52
00:03:44,858 --> 00:03:47,394
これを“Warp”と呼びます

53
00:03:48,328 --> 00:03:50,898
最終的な画像が生成されたら

54
00:03:50,998 --> 00:03:54,168
ヘッドセットに送り
表示します

55
00:03:55,202 --> 00:03:58,172
これでは
多数の処理が発生します

56
00:03:58,305 --> 00:04:01,208
対処法はないでしょうか

57
00:03:58,305 --> 00:04:01,208
対処法はないでしょうか

58
00:04:02,676 --> 00:04:07,047
アプリケーションに
MSAAを使用したい場合―

59
00:04:07,147 --> 00:04:10,784
両眼にそれぞれ
専用テクスチャが必要です

60
00:04:11,084 --> 00:04:13,520
１つを両眼で共有すると

61
00:04:13,620 --> 00:04:16,255
どちらも不完全になります

62
00:04:16,490 --> 00:04:22,029
専用テクスチャは 別々に
ドローコールが必要です

63
00:04:23,063 --> 00:04:26,333
一方
共有テクスチャの場合は

64
00:04:26,433 --> 00:04:29,436
レンダリングが１回で済みますが

65
00:04:30,204 --> 00:04:33,774
後処理効果で問題が生じます

66
00:04:34,975 --> 00:04:40,681
レイヤードテクスチャは
両方の利点を兼ね備えますが

67
00:04:41,148 --> 00:04:44,384
MSAAでは使えません

68
00:04:45,786 --> 00:04:50,057
このため MSAAを
用いるかどうかによって

69
00:04:50,157 --> 00:04:54,728
異なるレイアウトを
使わざるを得ません

70
00:04:54,862 --> 00:04:57,664
または周辺作業で
どうにかします

71
00:04:57,764 --> 00:05:00,467
これを最適化するには？

72
00:04:57,764 --> 00:05:00,467
これを最適化するには？

73
00:05:02,069 --> 00:05:04,771
そこで新たなテクスチャを
紹介します

74
00:05:04,905 --> 00:05:07,774
2Dマルチサンプル配列
テクスチャです

75
00:05:09,109 --> 00:05:13,580
これまで述べた
全ての利点を兼ね備え

76
00:05:13,680 --> 00:05:15,582
欠点もありません

77
00:05:16,550 --> 00:05:20,387
レンダリングスペースを
分けることができ

78
00:05:20,854 --> 00:05:25,392
それにより後処理効果を
単純化できます

79
00:05:26,460 --> 00:05:31,698
単一視点のレンダリングにも
容易に戻せます

80
00:05:33,167 --> 00:05:35,769
アンチエイリアスモードも
調節できます

81
00:05:36,703 --> 00:05:40,607
これにより 単一の
レンダリングファイルを

82
00:05:40,707 --> 00:05:44,811
あらゆる状況に
簡単に適用できます

83
00:05:44,912 --> 00:05:51,451
更にドローやレンダリングのパスが
１回で済む点も重要です

84
00:05:53,787 --> 00:05:59,626
このテクスチャを作成する
コードスニペットを見てみましょう

85
00:06:01,028 --> 00:06:03,063
sampleCountの設定は４です

86
00:06:03,163 --> 00:06:07,034
画質とパフォーマンスの
兼ね合いからです

87
00:06:07,134 --> 00:06:11,605
同時に arrayLengthは
２に設定しました

88
00:06:11,705 --> 00:06:17,544
両眼の画像を それぞれ
別に保存したいからです

89
00:06:18,912 --> 00:06:22,716
パイプラインは
どう変わるでしょうか

90
00:06:23,517 --> 00:06:26,887
2Dマルチサンプル
テクスチャを

91
00:06:27,154 --> 00:06:30,090
2Dマルチサンプル配列
テクスチャに替えました

92
00:06:33,026 --> 00:06:37,998
１つのパスで 両眼の
レンダリングが可能です

93
00:06:38,098 --> 00:06:43,170
インスタンシングを使えば
ドローコールも１回です

94
00:06:43,837 --> 00:06:47,541
しかし まだ
改善の余地があります

95
00:06:47,641 --> 00:06:50,978
2Dマルチサンプル配列
テクスチャを

96
00:06:51,078 --> 00:06:56,116
別々にIOSurfaceに
変換しなくてはなりません

97
00:06:57,484 --> 00:07:02,322
テクスチャをCompositorと
共有するには

98
00:06:57,484 --> 00:07:02,322
テクスチャをCompositorと
共有するには

99
00:07:02,723 --> 00:07:06,260
IOSurfaceを使えば
可能です

100
00:07:06,760 --> 00:07:12,366
異なるプロセスやGPU間で
テクスチャを共有できます

101
00:07:12,666 --> 00:07:15,836
しかし欠点もあります

102
00:07:16,436 --> 00:07:21,675
IOSurfaceで共有できるのは
単純な2Dテクスチャだけです

103
00:07:21,775 --> 00:07:26,613
そのためマルチサンプルを
用いる場合は

104
00:07:26,713 --> 00:07:28,081
共有できません

105
00:07:28,949 --> 00:07:32,486
そこで 共有可能な
Metalテクスチャの紹介です

106
00:07:32,586 --> 00:07:38,926
どのタイプのテクスチャも
プロセス間で共有可能になります

107
00:07:39,026 --> 00:07:43,430
ただし１つのGPU内の
テクスチャに限ります

108
00:07:45,332 --> 00:07:49,469
高度なVRへの
利用が可能になります

109
00:07:49,570 --> 00:07:54,274
例えば VR Compositorと
深度を共有できます

110
00:07:54,374 --> 00:07:56,944
それだけではありません

111
00:07:57,211 --> 00:08:00,547
テクスチャの作り方を
見てみましょう

112
00:07:57,211 --> 00:08:00,547
テクスチャの作り方を
見てみましょう

113
00:08:02,549 --> 00:08:08,055
複雑なテクスチャも
共有できるようになったので

114
00:08:08,155 --> 00:08:13,260
VR Compositorに渡す
2D配列テクスチャを作ります

115
00:08:14,194 --> 00:08:19,733
これには ご覧のような
新たなメソッドを用います

116
00:08:20,767 --> 00:08:25,906
storageModeは 忘れずに
Privateにしましょう

117
00:08:26,006 --> 00:08:31,178
テクスチャを作成したGPUのみ
アクセス可能です

118
00:08:33,780 --> 00:08:37,683
このコードスニペットを
見てください

119
00:08:37,784 --> 00:08:43,490
以前は このように
IOSurfaceを送っていました

120
00:08:44,091 --> 00:08:49,129
Metalテクスチャの使用に
切り替えるためには

121
00:08:50,030 --> 00:08:53,867
どんな変更を加えるか
見ていきます

122
00:08:54,868 --> 00:08:57,871
２つのIOSurfaceは
もう不要です

123
00:08:58,739 --> 00:09:01,475
IOSurface対応の
テクスチャは

124
00:08:58,739 --> 00:09:01,475
IOSurface対応の
テクスチャは

125
00:09:01,575 --> 00:09:05,779
１つのMetalテクスチャに
置き換えます

126
00:09:05,879 --> 00:09:07,915
2D配列型です

127
00:09:09,449 --> 00:09:14,454
２つのtextureDescriptorに
これを割り当てます

128
00:09:14,855 --> 00:09:17,124
オープンソースの
VR SDKです

129
00:09:17,624 --> 00:09:21,795
タイプをMetalに変更します

130
00:09:22,462 --> 00:09:25,432
いくつかの変更を加えました

131
00:09:25,732 --> 00:09:32,005
これで左右の眼の画像を
Compositorに提出できます

132
00:09:32,239 --> 00:09:34,841
これによりCompositorは

133
00:09:34,942 --> 00:09:39,413
Metalテクスチャを
渡されたと理解します

134
00:09:39,813 --> 00:09:45,018
テクスチャのタイプを
チェックしてみると

135
00:09:45,152 --> 00:09:48,989
Compositorが自動で判断します

136
00:09:49,089 --> 00:09:52,759
左眼の画像は
スライス０に保管され

137
00:09:52,860 --> 00:09:56,230
右眼はスライス１に
保管されています

138
00:09:56,463 --> 00:09:59,700
アプリケーションは
何もしなくてよいのです

139
00:10:01,301 --> 00:10:05,672
アプリケーションと
Compositorの共有だけが

140
00:10:05,772 --> 00:10:09,109
Metalテクスチャの
特徴ではありません

141
00:10:09,343 --> 00:10:15,282
プロセス間でテクスチャを
渡す例を見てみましょう

142
00:10:16,083 --> 00:10:20,220
最初は同じように
Metalテクスチャを作ります

143
00:10:20,621 --> 00:10:26,860
今回は このテクスチャから
テクスチャハンドルを作ります

144
00:10:26,960 --> 00:10:32,799
XPC接続により プロセス間で
渡すことができます

145
00:10:33,133 --> 00:10:37,271
もう一方のプロセスに
ハンドルを渡せば

146
00:10:37,838 --> 00:10:41,875
これを用いてオブジェクトを
再形成できます

147
00:10:42,242 --> 00:10:44,211
ここで注意点があります

148
00:10:44,444 --> 00:10:48,115
テクスチャオブジェクトの
再形成は

149
00:10:48,215 --> 00:10:53,687
それが作られたのと
同じデバイス上で行いましょう

150
00:10:53,787 --> 00:10:58,025
GPUの適用範囲を
超えられないためです

151
00:11:00,494 --> 00:11:03,597
では パイプラインに
戻りましょう

152
00:11:04,331 --> 00:11:05,966
どう変わるでしょう？

153
00:11:06,233 --> 00:11:10,103
アプリケーションでは
２つのIOSurfaceが

154
00:11:10,204 --> 00:11:14,675
１つの2D配列テクスチャに
入れ替わりました

155
00:11:15,275 --> 00:11:17,811
これで大幅に最適化できます

156
00:11:17,911 --> 00:11:22,182
2Dマルチサンプル
配列テクスチャは

157
00:11:22,282 --> 00:11:27,254
ワンパスで共有テクスチャに
変換されます

158
00:11:27,588 --> 00:11:31,124
Compositorに注目してみましょう

159
00:11:32,159 --> 00:11:35,762
アプリケーション側の
簡便化により

160
00:11:35,863 --> 00:11:40,901
Compositor側も新機能を
存分に活用できます

161
00:11:41,201 --> 00:11:47,474
Compositorは
渡された2D配列テクスチャを

162
00:11:47,574 --> 00:11:51,945
１回のレンダリングパスで
両眼に使えます

163
00:11:53,247 --> 00:11:56,817
パイプライン全体が
簡略化されました

164
00:11:57,684 --> 00:12:00,120
おさらいしましょう

165
00:11:57,684 --> 00:12:00,120
おさらいしましょう

166
00:12:01,655 --> 00:12:04,424
Metalの新機能は２つ

167
00:12:04,525 --> 00:12:07,394
共有可能なMetalテクスチャと

168
00:12:07,494 --> 00:12:10,063
2Dマルチサンプル配列
テクスチャです

169
00:12:10,564 --> 00:12:15,102
パイプライン最適化の
方法も話しました

170
00:12:15,969 --> 00:12:20,440
SteamVRランタイムの
アップデートで対応します

171
00:12:22,576 --> 00:12:25,846
次は アプリケーションで
CPUとGPUを

172
00:12:25,946 --> 00:12:28,815
最大限に活用する方法です

173
00:12:30,083 --> 00:12:33,520
２つに分けて お話ししましょう

174
00:12:33,620 --> 00:12:36,957
高度なフレームペーシングと
フィルレートの軽減です

175
00:12:38,759 --> 00:12:40,727
まず フレームペーシングです

176
00:12:41,562 --> 00:12:45,299
アプリケーションの
フレームペーシングを分析し

177
00:12:45,399 --> 00:12:47,901
最適化する方法をお話しします

178
00:12:48,202 --> 00:12:54,775
全てを連続的に実行する
シンプルな例で説明しましょう

179
00:12:55,375 --> 00:13:00,247
まず WaitGetPosesを呼び出し
フレームを開始します

180
00:12:55,375 --> 00:13:00,247
まず WaitGetPosesを呼び出し
フレームを開始します

181
00:13:00,781 --> 00:13:02,482
Poseを受け取り

182
00:13:02,916 --> 00:13:07,554
ヘッドセットのフレームレートに
同期して実行します

183
00:13:09,022 --> 00:13:14,094
Viveも Vive Proも
リフレッシュレートは90です

184
00:13:14,328 --> 00:13:19,633
１フレームの処理時間は
11.1ミリ秒ということです

185
00:13:20,067 --> 00:13:25,038
ちなみに瞬き１回の時間は
300ミリ秒です

186
00:13:25,539 --> 00:13:28,976
この間に50フレーム
描画します

187
00:13:30,277 --> 00:13:34,414
アプリケーションが
Poseを受け取ると

188
00:13:34,515 --> 00:13:38,051
シミュレーションを
開始できます

189
00:13:38,685 --> 00:13:42,890
完了し 全オブジェクトの
状態が分かれば

190
00:13:42,990 --> 00:13:47,361
コマンドバッファの
エンコードを継続できます

191
00:13:47,461 --> 00:13:50,631
これがGPUに送られ
実行されます

192
00:13:51,798 --> 00:13:56,770
GPUで実行され
両眼の画像が描画されると

193
00:13:56,870 --> 00:14:03,010
VR Compositorに送られ
最終的な後処理が行われます

194
00:13:56,870 --> 00:14:03,010
VR Compositorに送られ
最終的な後処理が行われます

195
00:14:05,012 --> 00:14:09,983
その後 フレームが
ヘッドセットへ読み込まれます

196
00:14:10,517 --> 00:14:16,156
画像を表示する前に
全ピクセルが更新されるため

197
00:14:16,256 --> 00:14:18,425
もう１フレーム必要です

198
00:14:19,226 --> 00:14:21,795
ピクセルが更新されると

199
00:14:22,262 --> 00:14:26,400
パネルから光子が放出され
フレームが見られます

200
00:14:27,234 --> 00:14:30,637
アプリケーションが
Poseを受け取ってから

201
00:14:30,804 --> 00:14:36,343
画像が出るまでに要するのは
約25ミリ秒です

202
00:14:37,110 --> 00:14:40,280
そのためアプリケーションは

203
00:14:40,380 --> 00:14:43,350
先を予測したPoseを
受け取ります

204
00:14:43,450 --> 00:14:45,919
光子放出の瞬間を予測し

205
00:14:46,019 --> 00:14:50,357
描画される画像が
合致するようにします

206
00:14:52,226 --> 00:14:57,030
この流れは前後のフレームと
重なり合い

207
00:14:57,130 --> 00:14:59,833
ダイアグラムを
形成しています

208
00:15:00,567 --> 00:15:04,037
このような
シングルスレッドの例では

209
00:15:04,471 --> 00:15:06,707
GPUのアイドル状態が
長いです

210
00:15:07,741 --> 00:15:10,043
これをどうにかしましょう

211
00:15:12,146 --> 00:15:15,516
マルチスレッドに
切り替えます

212
00:15:15,616 --> 00:15:19,853
視覚環境の
シミュレーションと

213
00:15:19,953 --> 00:15:23,056
GPU操作のエンコードを
分けました

214
00:15:23,390 --> 00:15:27,661
エンコードは別のスレッドで
発生します

215
00:15:28,695 --> 00:15:31,698
こうして分けたことにより

216
00:15:31,798 --> 00:15:35,269
シミュレーションと並行して

217
00:15:36,036 --> 00:15:39,740
前フレームのエンコードができます

218
00:15:40,574 --> 00:15:44,945
つまり エンコードの
開始時期が変わり

219
00:15:45,045 --> 00:15:49,016
Poseを受け取って
すぐに開始されます

220
00:15:49,116 --> 00:15:54,221
アプリケーションが
エンコードに使える時間が増え

221
00:15:54,321 --> 00:15:57,658
GPUの処理時間も増えます

222
00:15:57,758 --> 00:16:01,461
その結果 見た目が向上します

223
00:15:57,758 --> 00:16:01,461
その結果 見た目が向上します

224
00:16:02,763 --> 00:16:04,665
ただし注意点があります

225
00:16:05,332 --> 00:16:09,636
シミュレーションが
１フレーム前に起こるため

226
00:16:10,037 --> 00:16:13,740
予測されたPoseも別途必要です

227
00:16:14,041 --> 00:16:18,645
このセットでは
56ミリ秒先を予測し

228
00:16:18,745 --> 00:16:22,249
レンダリングスレッドの
予測セットを

229
00:16:22,349 --> 00:16:25,552
光子放出の瞬間に
合致させています

230
00:16:27,054 --> 00:16:30,023
CPUに関しては
これで問題ありません

231
00:16:30,123 --> 00:16:35,729
アプリケーションが うまく
作業を分布させています

232
00:16:36,129 --> 00:16:37,898
GPUはどうでしょう

233
00:16:40,200 --> 00:16:44,838
このアプリケーションの
例を見てください

234
00:16:45,038 --> 00:16:48,642
フレーム全体のGPUの処理を

235
00:16:48,742 --> 00:16:52,045
１つのコマンドバッファに
エンコードしています

236
00:16:52,679 --> 00:16:57,651
コマンドバッファの完了まで
GPUはアイドル状態です

237
00:16:58,719 --> 00:17:03,757
しかしCPUがエンコードに
要する時間は

238
00:16:58,719 --> 00:17:03,757
しかしCPUがエンコードに
要する時間は

239
00:17:03,857 --> 00:17:08,494
GPUの操作にかかる時間より
はるかに短いのです

240
00:17:08,962 --> 00:17:10,931
この事実を利用します

241
00:17:11,265 --> 00:17:16,537
エンコードを いくつかの
コマンドバッファに分けます

242
00:17:16,637 --> 00:17:21,575
分かれたコマンドバッファは
迅速にエンコードされ

243
00:17:21,675 --> 00:17:24,711
すぐにGPUに渡されます

244
00:17:25,579 --> 00:17:26,780
こうすれば―

245
00:17:27,981 --> 00:17:34,922
エンコードと並行して
GPUがフレームを処理できます

246
00:17:35,022 --> 00:17:41,295
GPUが処理に使える時間も
増やすことができました

247
00:17:41,862 --> 00:17:47,034
１フレームあたりの処理量も
大幅に増やせます

248
00:17:48,068 --> 00:17:52,139
では ダイヤグラム全体を
見てみましょう

249
00:17:53,340 --> 00:17:58,278
CPUとGPUを
最大限に活用できています

250
00:17:58,712 --> 00:18:03,183
この時点で
かなり よい見本ですが

251
00:17:58,712 --> 00:18:03,183
この時点で
かなり よい見本ですが

252
00:18:03,484 --> 00:18:05,752
もっと改善できます

253
00:18:07,087 --> 00:18:11,291
レンダリングスレッドには
まだ待ち時間があります

254
00:18:11,391 --> 00:18:17,397
Poseを受け取るまで
エンコードを待っている状態です

255
00:18:17,498 --> 00:18:21,869
しかしPoseを
必要としない処理もあります

256
00:18:23,003 --> 00:18:26,507
もっと詳しく見てみましょう

257
00:18:28,408 --> 00:18:33,247
各フレームで実行される
処理のリストです

258
00:18:33,947 --> 00:18:36,383
一部はスクリーンで
発生します

259
00:18:36,483 --> 00:18:41,088
Poseの情報を
必要とするものもあります

260
00:18:41,388 --> 00:18:44,391
これをPose依存処理と呼びます

261
00:18:44,992 --> 00:18:48,362
一方で 一般的な処理もあり

262
00:18:48,462 --> 00:18:52,366
Poseの情報なしで
すぐ実行できます

263
00:18:53,000 --> 00:18:56,136
これを独立処理と呼びます

264
00:18:57,638 --> 00:19:03,343
先ほど アプリケーションは
Poseを待っていましたが

265
00:18:57,638 --> 00:19:03,343
先ほど アプリケーションは
Poseを待っていましたが

266
00:19:03,510 --> 00:19:06,280
この処理を半分に分けると

267
00:19:07,247 --> 00:19:10,918
独立処理を すぐ
エンコードできます

268
00:19:11,018 --> 00:19:15,455
その後 Pose依存処理の
エンコードを待ちます

269
00:19:17,491 --> 00:19:21,662
このスライドでは
独立処理と依存処理を

270
00:19:21,762 --> 00:19:23,630
すでに分けています

271
00:19:24,398 --> 00:19:28,268
独立処理をエンコードする
コマンドバッファは

272
00:19:28,368 --> 00:19:33,440
Pose依存処理よりも
濃い色で示しています

273
00:19:34,241 --> 00:19:38,011
独立処理は
すぐエンコードできるので

274
00:19:38,145 --> 00:19:43,617
このように 前フレームの直後に
エンコードします

275
00:19:44,651 --> 00:19:48,388
これでCPUの時間を
増やせます

276
00:19:48,589 --> 00:19:52,226
すでに このGPUの処理が―

277
00:19:52,326 --> 00:19:57,130
実行を待っていることが
重要な点です

278
00:19:57,698 --> 00:20:00,434
アイドルタイムはありません

279
00:19:57,698 --> 00:20:00,434
アイドルタイムはありません

280
00:20:00,534 --> 00:20:04,404
前フレームが終わり次第
次が開始します

281
00:20:06,907 --> 00:20:11,712
最後のテーマは
マルチGPUの処理の分散です

282
00:20:13,113 --> 00:20:16,750
複数のGPUに
処理を分けるのです

283
00:20:17,217 --> 00:20:19,953
MacBook Proは
GPUが２つあります

284
00:20:20,053 --> 00:20:25,692
２つの性能は異なりますが
使用に問題はありません

285
00:20:26,093 --> 00:20:30,130
同様に アプリケーションでも
両方を使えます

286
00:20:30,230 --> 00:20:34,434
MacのプライマリGPUで
負担を減らしながら

287
00:20:34,668 --> 00:20:36,303
レンダリングできます

288
00:20:39,039 --> 00:20:45,579
そこで 独立処理を分離して
第２のGPUに移行しました

289
00:20:45,679 --> 00:20:49,783
前のフレームで
エンコード済みなので

290
00:20:49,883 --> 00:20:52,786
この処理が可能なのです

291
00:20:52,886 --> 00:20:55,289
この独立処理と並行して

292
00:20:55,556 --> 00:20:59,993
前フレームのPose依存処理が
実行されます

293
00:21:00,094 --> 00:21:05,132
これで更に
GPUの時間を増やせます

294
00:21:07,634 --> 00:21:11,071
しかし
処理を分割するためには

295
00:21:11,705 --> 00:21:14,641
必要なことがあります

296
00:21:14,741 --> 00:21:17,778
処理を互いに
同期させることです

297
00:21:23,016 --> 00:21:28,288
ここで 新しい
同期プリミティブを紹介します

298
00:21:28,722 --> 00:21:32,893
MTLEventを使えば
１つのGPUの範囲内で

299
00:21:32,993 --> 00:21:36,730
異なるキューの処理を
同期できます

300
00:21:37,097 --> 00:21:41,068
更に MTLSharedEventは

301
00:21:41,168 --> 00:21:44,771
異なるGPUの処理を
同期できます

302
00:21:44,872 --> 00:21:47,174
異なるプロセス間も可能です

303
00:21:49,109 --> 00:21:52,779
では 単純なコードを
見てみましょう

304
00:21:53,447 --> 00:21:58,485
Thunderbolt 3で
MacにeGPUを接続しました

305
00:21:58,752 --> 00:22:02,956
ヘッドセットを動かす
プライマリGPUです

306
00:21:58,752 --> 00:22:02,956
ヘッドセットを動かす
プライマリGPUです

307
00:22:03,357 --> 00:22:09,429
すでにMacに入っているGPUは
補助として使います

308
00:22:10,397 --> 00:22:15,702
SharedEventで
両GPUの処理を同期します

309
00:22:19,106 --> 00:22:21,542
Eventの初期値は０なので

310
00:22:21,642 --> 00:22:25,812
同期カウンタは
１から始めることが重要です

311
00:22:26,113 --> 00:22:30,050
なぜなら 初期化された
Eventを待つ時

312
00:22:30,150 --> 00:22:36,256
カウンタが０だと戻ってしまい
同期されないからです

313
00:22:37,758 --> 00:22:40,494
補助GPUのエンコード処理は

314
00:22:40,594 --> 00:22:44,198
レンダリングスレッドで
すぐ開始されます

315
00:22:44,431 --> 00:22:50,370
補助GPU上で発生する
独立処理のエンコードです

316
00:22:50,470 --> 00:22:52,506
この処理が完了すると

317
00:22:53,106 --> 00:22:56,477
ローカルメモリに保存されます

318
00:22:56,777 --> 00:23:00,948
そのため 続けて
短い操作をエンコードします

319
00:22:56,777 --> 00:23:00,948
そのため 続けて
短い操作をエンコードします

320
00:23:01,048 --> 00:23:03,383
両GPUから見えるよう

321
00:23:03,517 --> 00:23:07,120
結果をシステムメモリに
転送するのです

322
00:23:07,354 --> 00:23:09,790
転送が完了すれば

323
00:23:10,524 --> 00:23:15,562
補助GPUがSharedEventに
信号を送り

324
00:23:15,796 --> 00:23:20,634
結果を安全に取得できると
eGPUに知らせます

325
00:23:22,035 --> 00:23:25,772
レンダリングスレッドが
コマンドバッファをコミットし

326
00:23:25,873 --> 00:23:29,676
補助GPUは
すでに処理を始めています

327
00:23:29,943 --> 00:23:33,714
同時にプライマリGPUの
コマンドバッファの

328
00:23:33,814 --> 00:23:36,783
エンコードを開始できます

329
00:23:37,584 --> 00:23:42,022
まず SharedEventが
システムメモリのデータを

330
00:23:42,122 --> 00:23:44,758
確認するのを待ちます

331
00:23:44,858 --> 00:23:48,428
確認でき SharedEventが
信号を送れば

332
00:23:48,629 --> 00:23:54,101
eGPUにデータを転送する操作を

333
00:23:54,201 --> 00:23:56,803
実行することができます

334
00:23:57,337 --> 00:24:02,509
転送が済めば Pose依存処理が
安全に行えます

335
00:23:57,337 --> 00:24:02,509
転送が済めば Pose依存処理が
安全に行えます

336
00:24:02,943 --> 00:24:07,681
第２のコマンドバッファが
信号を送り

337
00:24:07,815 --> 00:24:11,919
Pose依存処理が
実行可能と知らせます

338
00:24:12,252 --> 00:24:16,190
２つのコマンドバッファの
提出が済みました

339
00:24:16,290 --> 00:24:18,892
レンダリングスレッドは引き続き

340
00:24:18,992 --> 00:24:23,163
Pose依存処理を
エンコードできます

341
00:24:26,166 --> 00:24:32,105
このように 異なるGPUの
処理を同期できましたが

342
00:24:33,907 --> 00:24:38,145
第２のGPUには
まだ余力があります

343
00:24:38,579 --> 00:24:44,151
なぜなら この例で
第２のGPUに課したのは

344
00:24:44,251 --> 00:24:50,124
Pose依存処理に依存する
独立処理だからです

345
00:24:50,724 --> 00:24:54,428
しかし 依存性のない
処理もあり

346
00:24:54,528 --> 00:24:59,132
比較的低い頻度で発生します

347
00:24:59,466 --> 00:25:02,603
このような処理の
一例を挙げると

348
00:24:59,466 --> 00:25:02,603
このような処理の
一例を挙げると

349
00:25:02,736 --> 00:25:07,541
物理ベースの
正確なシミュレーション

350
00:25:07,641 --> 00:25:11,745
その他 更新に
時間を要するものなどです

351
00:25:13,213 --> 00:25:16,717
このような処理は
バックグラウンドで

352
00:25:16,817 --> 00:25:21,355
フレームとは非同期的に
その都度 発生し

353
00:25:21,889 --> 00:25:24,858
結果はプライマリGPUに送られます

354
00:25:25,492 --> 00:25:31,331
特定のフレームと無関係の処理を
グレーで示しました

355
00:25:32,399 --> 00:25:37,704
もちろん性能特性の異なる
さまざまなGPUがあり

356
00:25:37,805 --> 00:25:40,641
接続の帯域幅も異なります

357
00:25:41,442 --> 00:25:45,546
アプリケーションでは
１フレームの中に

358
00:25:45,646 --> 00:25:48,182
さまざまに関連し合う
処理があります

359
00:25:48,882 --> 00:25:54,121
この処理をどう配分するか
考える必要があります

360
00:25:54,855 --> 00:25:57,591
そこで まず重要なのは

361
00:25:57,691 --> 00:26:00,928
GPUの処理の配分を
考えることです

362
00:25:57,691 --> 00:26:00,928
GPUの処理の配分を
考えることです

363
00:26:01,028 --> 00:26:06,366
AppleではマルチGPUが
一般的になりつつあります

364
00:26:08,569 --> 00:26:12,072
では ここまでのおさらいです

365
00:26:12,406 --> 00:26:17,778
マルチスレッドで
CPUコードを最大限に活用し

366
00:26:18,846 --> 00:26:23,050
コマンドバッファを分割し
GPUの無駄をなくします

367
00:26:23,650 --> 00:26:25,953
その際に できれば

368
00:26:26,053 --> 00:26:30,090
Pose依存処理と
独立処理を分けましょう

369
00:26:30,657 --> 00:26:33,994
なるべく迅速に
エンコードするためです

370
00:26:34,461 --> 00:26:39,066
そして 更新頻度により
処理を分けます

371
00:26:39,800 --> 00:26:43,504
マルチGPUで
アプリケーションを実行する時

372
00:26:43,604 --> 00:26:47,841
簡単に 各GPUに
割り振ることができます

373
00:26:48,175 --> 00:26:49,510
その時には

374
00:26:49,943 --> 00:26:55,048
別のレンダリングスレッドで
各GPUを動かしましょう

375
00:26:55,382 --> 00:26:58,519
非同期に実行するためです

376
00:26:59,019 --> 00:27:01,422
次はフィルレートの話です

377
00:26:59,019 --> 00:27:01,422
次はフィルレートの話です

378
00:27:03,190 --> 00:27:07,861
Vive Proがもたらす
新たな課題です

379
00:27:07,961 --> 00:27:13,066
この問題を理解するため
比較してみましょう

380
00:27:14,201 --> 00:27:16,637
アプリケーションの
レンダリングでは

381
00:27:16,803 --> 00:27:20,874
Viveヘッドセットの場合 既定値で

382
00:27:21,275 --> 00:27:25,546
フィルレートは
毎秒436メガピクセルです

383
00:27:27,381 --> 00:27:31,418
最先端の
4KウルトラHD TVの

384
00:27:32,019 --> 00:27:36,290
フィルレートと
比較してみましょう

385
00:27:36,390 --> 00:27:40,460
毎秒475メガピクセルです

386
00:27:40,828 --> 00:27:43,931
これだけでも すでに大きく

387
00:27:44,031 --> 00:27:47,835
開発者はフィルレート軽減に
苦労します

388
00:27:48,135 --> 00:27:51,538
しかし Vive Proは
どうでしょう

389
00:27:53,006 --> 00:27:58,946
フィルレートは 通常でも
毎秒775メガピクセルです

390
00:27:59,046 --> 00:28:03,016
更に4xMSAAなどを追加するなら

391
00:27:59,046 --> 00:28:03,016
更に4xMSAAなどを追加するなら

392
00:28:03,116 --> 00:28:06,820
この数字は より大きくなります

393
00:28:08,055 --> 00:28:11,525
フィルレートの軽減が
重要なのです

394
00:28:12,025 --> 00:28:16,096
さまざまな技が 日々
生み出されています

395
00:28:16,196 --> 00:28:19,666
全て試すことを
お勧めしますが

396
00:28:20,234 --> 00:28:23,670
今日は いくつかに
焦点を当てます

397
00:28:23,904 --> 00:28:30,144
簡単に実行することができ
優れた結果が得られます

398
00:28:30,444 --> 00:28:33,413
まず ピクセルの
クリッピングです

399
00:28:34,481 --> 00:28:37,484
これは左眼用の画像です

400
00:28:38,452 --> 00:28:41,755
しかし レンズの性質により

401
00:28:42,589 --> 00:28:46,493
約20パーセントのピクセルは
失われます

402
00:28:46,593 --> 00:28:50,464
Compositorによる
ゆがみ補正のあと―

403
00:28:50,697 --> 00:28:54,968
右の画像がヘッドセットの
パネルに表示されます

404
00:28:55,068 --> 00:28:57,137
その後 レンズを通ります

405
00:28:58,639 --> 00:29:01,942
簡単にフィルレートを
減らすには

406
00:28:58,639 --> 00:29:01,942
簡単にフィルレートを
減らすには

407
00:29:02,342 --> 00:29:07,848
最終的に見えないピクセルは
描画しないことです

408
00:29:07,948 --> 00:29:12,653
SteamVRのステンシルマスクで
簡単にできます

409
00:29:15,255 --> 00:29:21,195
この方法で フィルレートを
20パーセント軽減でき

410
00:29:21,295 --> 00:29:26,433
Vive Proの場合で
620メガピクセルに減りました

411
00:29:29,269 --> 00:29:30,104
さて―

412
00:29:30,504 --> 00:29:35,709
レンズの ゆがみ補正を
更に詳しく分析します

413
00:29:37,544 --> 00:29:38,846
視野を分割して

414
00:29:39,746 --> 00:29:43,317
９つのセクションに分けます

415
00:29:43,851 --> 00:29:48,222
中央の視野角は
水平方向に80度

416
00:29:48,322 --> 00:29:50,591
垂直方向に80度です

417
00:29:50,891 --> 00:29:54,795
その周囲に辺と角の
領域があります

418
00:29:55,195 --> 00:29:57,064
この部分に色をつけ

419
00:29:57,297 --> 00:30:01,001
最終像との関係を
可視化しました

420
00:29:57,297 --> 00:30:01,001
最終像との関係を
可視化しました

421
00:30:02,836 --> 00:30:07,341
このように
角は ほぼ見えません

422
00:30:07,441 --> 00:30:11,178
そして辺の部分も―

423
00:30:11,278 --> 00:30:15,082
元の画像より
はるかに狭くなります

424
00:30:15,382 --> 00:30:19,019
実は ヘッドセットで見る時

425
00:30:19,153 --> 00:30:23,724
赤の部分を
直接見ることはできません

426
00:30:24,091 --> 00:30:27,761
周辺視野として見えるだけです

427
00:30:28,829 --> 00:30:31,999
これが大きなヒントとなります

428
00:30:33,200 --> 00:30:38,572
この辺と角のフィルレートは
減らすことが可能です

429
00:30:39,106 --> 00:30:41,375
ほぼ見えないからです

430
00:30:42,142 --> 00:30:46,246
中央は これまでどおりに
描画します

431
00:30:47,247 --> 00:30:51,251
左右の辺の部分は
幅を半分にし

432
00:30:51,552 --> 00:30:54,988
上下の辺は
高さを半分にします

433
00:30:55,455 --> 00:31:01,462
そして角の部分の描画は
解像度を４分の１にします

434
00:30:55,455 --> 00:31:01,462
そして角の部分の描画は
解像度を４分の１にします

435
00:31:02,996 --> 00:31:09,369
レンダリングパスが完了したあと
アップスケーリングします

436
00:31:09,469 --> 00:31:15,642
辺縁の領域を伸ばして
元の解像度に戻すのです

437
00:31:16,610 --> 00:31:19,746
どのくらいの効果があるか？

438
00:31:21,548 --> 00:31:26,320
中央領域が上下左右80度なら
フィルレートは

439
00:31:26,420 --> 00:31:30,891
毎秒491メガピクセルまで
減らせます

440
00:31:31,658 --> 00:31:35,629
見えないピクセルを
切り取る技を

441
00:31:35,729 --> 00:31:38,365
これと併用してみましょう

442
00:31:39,933 --> 00:31:43,737
ピクセルのクリッピングと
合わせると

443
00:31:43,837 --> 00:31:49,543
フィルレートは更に減り
毎秒456メガピクセルになります

444
00:31:49,643 --> 00:31:51,445
実は この数字は

445
00:31:51,578 --> 00:31:56,116
Viveヘッドセットの
既定値なのです

446
00:31:56,583 --> 00:31:59,820
つまり この２つの技を使えば

447
00:31:59,920 --> 00:32:04,658
Viveの時と
同じGPUを用いながら

448
00:31:59,920 --> 00:32:04,658
Viveの時と
同じGPUを用いながら

449
00:32:04,758 --> 00:32:10,097
はるかに解像度の高い
Vive Proに描画できます

450
00:32:10,697 --> 00:32:14,501
もちろん Viveにも
この技が使えます

451
00:32:14,601 --> 00:32:18,539
それによって
アプリケーションの見た目が

452
00:32:18,639 --> 00:32:21,041
大きく向上します

453
00:32:23,410 --> 00:32:25,512
ただし注意点があります

454
00:32:25,612 --> 00:32:29,516
多重解像度シェーディングは
レンダリングパスが少なく

455
00:32:30,050 --> 00:32:35,656
ジオメトリパイプラインの
処理が増大します

456
00:32:36,089 --> 00:32:42,129
これを緩和するには
中央の視野角を少し減らします

457
00:32:42,463 --> 00:32:46,300
このように10度 減らすだけでも

458
00:32:46,400 --> 00:32:51,038
毎秒382メガピクセルまで
軽減できます

459
00:32:51,438 --> 00:32:54,875
処理量が極めて大きい場合

460
00:32:55,342 --> 00:32:59,313
もっと領域を狭くして
試してみましょう

461
00:32:59,413 --> 00:33:04,384
更にフィルレートを
下げることができます

462
00:32:59,413 --> 00:33:04,384
更にフィルレートを
下げることができます

463
00:33:04,751 --> 00:33:08,489
中央領域が
上下左右55度の場合

464
00:33:08,622 --> 00:33:14,561
視線の動きの80パーセントが
この領域内に収まります

465
00:33:15,262 --> 00:33:18,432
しかし フィルレートは
半分以上減り

466
00:33:18,532 --> 00:33:21,802
毎秒360メガピクセルに
なります

467
00:33:23,570 --> 00:33:27,875
もちろん 他にも方法は
いろいろあります

468
00:33:29,009 --> 00:33:34,114
それにより得られる効果も
さまざまでしょう

469
00:33:34,681 --> 00:33:39,853
どれが最適か
いろいろ試してみてください

470
00:33:41,422 --> 00:33:44,758
では 今回のセッションの
まとめです

471
00:33:46,994 --> 00:33:51,298
まず Vive Proの
プラグアンドプレイ対応

472
00:33:51,765 --> 00:33:55,802
そして よりよい
VRアプリケーションのための

473
00:33:55,903 --> 00:33:59,073
Metal 2の新機能を
紹介しました

474
00:33:59,606 --> 00:34:04,378
マルチGPUの活用も
ぜひ試してください

475
00:33:59,606 --> 00:34:04,378
マルチGPUの活用も
ぜひ試してください

476
00:34:04,478 --> 00:34:07,815
Appleでは一般的に
なりつつあります

477
00:34:10,083 --> 00:34:13,587
更に詳しい情報が
リンク先にあります

478
00:34:13,754 --> 00:34:18,592
テクノロジーラボで
本日12時から実施予定の

479
00:34:18,692 --> 00:34:24,831
Metal 4 VRラボにも
ぜひ ご参加ください

480
00:34:26,132 --> 00:34:26,867
ありがとうございました

481
00:34:27,234 --> 00:34:31,638
(拍手)