
1
00:00:07,207 --> 00:00:14,314
(音楽)

2
00:00:21,822 --> 00:00:25,893
(拍手)

3
00:00:26,426 --> 00:00:27,828
こんにちは

4
00:00:27,928 --> 00:00:30,264
ここで話す内容は

5
00:00:30,364 --> 00:00:35,269
ARKitの追跡や
認識についてです

6
00:00:35,369 --> 00:00:40,040
ぜひ ARを構築してください

7
00:00:40,574 --> 00:00:43,610
私は ARKitチームの
マリオンです

8
00:00:44,011 --> 00:00:45,145
皆さんは？

9
00:00:46,113 --> 00:00:51,752
ARKitの構成が知りたい
デベロッパでしょうか？

10
00:00:52,219 --> 00:00:53,954
期待に応えましょう

11
00:00:55,255 --> 00:00:57,024
初心者かもしれません

12
00:00:57,491 --> 00:01:01,128
ここで さまざまな
追跡に関する技術や

13
00:00:57,491 --> 00:01:01,128
ここで さまざまな
追跡に関する技術や

14
00:01:01,228 --> 00:01:05,632
ARの基礎や専門用語を
学ぶことができます

15
00:01:05,732 --> 00:01:10,204
ARを初めて構築するのに
役立つでしょう

16
00:01:11,872 --> 00:01:13,207
では 始めます

17
00:01:13,740 --> 00:01:14,875
追跡とは？

18
00:01:18,011 --> 00:01:21,515
追跡は 現実世界について

19
00:01:21,615 --> 00:01:25,252
カメラが認識すべき
位置を提供します

20
00:01:25,352 --> 00:01:30,424
すると カメラ越しに
仮想コンテンツを付与できます

21
00:01:31,124 --> 00:01:33,227
ビデオで説明しましょう

22
00:01:33,527 --> 00:01:37,931
手前のテーブルとイスは
仮想コンテンツです

23
00:01:38,031 --> 00:01:42,135
現実世界のテラスに
情報を付与しています

24
00:01:42,703 --> 00:01:44,571
IKEAの製品です

25
00:01:45,405 --> 00:01:49,443
仮想コンテンツは常に
正確に表示されます

26
00:01:49,710 --> 00:01:54,882
正確な配置 サイズ
見え方で表示されるのです

27
00:01:55,749 --> 00:02:01,655
多様な追跡技術が カメラに
参照用のシステムを提供します

28
00:01:55,749 --> 00:02:01,655
多様な追跡技術が カメラに
参照用のシステムを提供します

29
00:02:01,755 --> 00:02:04,458
つまり カメラが現実世界や

30
00:02:04,558 --> 00:02:08,428
情報を捉えるということです

31
00:02:09,128 --> 00:02:14,301
他の追跡技術については
この後に説明します

32
00:02:14,968 --> 00:02:19,106
そうすれば
用途に合った選択ができます

33
00:02:20,908 --> 00:02:26,180
まず話すのは 既存の
AR技術である位置特定や

34
00:02:26,346 --> 00:02:29,416
方向特定や
平面検出についてです

35
00:02:29,583 --> 00:02:35,556
そのあと 最新の認識技術や
追跡技術を備えた―

36
00:02:36,223 --> 00:02:38,525
ARKit 2を説明します

37
00:02:38,659 --> 00:02:44,064
マップの保存やロード
画像追跡や物体検出の説明です

38
00:02:45,299 --> 00:02:47,901
これらの詳細を説明する前に

39
00:02:48,368 --> 00:02:52,973
ARKitを手短に
復習しましょう

40
00:02:53,173 --> 00:02:56,643
初心者の方には
興味深い内容です

41
00:02:58,378 --> 00:03:02,149
まず ARSessionを
作成します

42
00:02:58,378 --> 00:03:02,149
まず ARSessionを
作成します

43
00:03:02,449 --> 00:03:05,285
ARSessionは
オブジェクトで―

44
00:03:05,953 --> 00:03:11,391
AR技術の構成から稼働まで
全てを処理します

45
00:03:11,492 --> 00:03:15,996
さらに AR技術の
処理結果も返します

46
00:03:17,097 --> 00:03:22,069
そのあと どの技術を
起動するか指示します

47
00:03:22,169 --> 00:03:24,404
例えば
どの追跡技術を使うか

48
00:03:24,505 --> 00:03:28,408
方向特定を有効にするか
などを指示します

49
00:03:30,410 --> 00:03:34,047
ARConfigurationを
取得して

50
00:03:34,181 --> 00:03:40,387
ARSession上の
メソッドを呼び出します

51
00:03:41,522 --> 00:03:43,557
すると ARSessionは

52
00:03:44,224 --> 00:03:48,962
AVCaptureSessionの
構成を開始

53
00:03:49,096 --> 00:03:51,498
画像の受信を始めます

54
00:03:51,598 --> 00:03:57,437
CMMotionManagerも
同様に構成されます

55
00:03:57,538 --> 00:04:03,143
これが基本的な ARKitに
内蔵された入力システムです

56
00:03:57,538 --> 00:04:03,143
これが基本的な ARKitに
内蔵された入力システムです

57
00:04:05,112 --> 00:04:11,151
処理結果はARFrameに
毎秒60フレームで返ります

58
00:04:12,052 --> 00:04:18,125
ARFrameはレンダリングに
必要な情報を与えてくれます

59
00:04:18,458 --> 00:04:21,862
例えば
カメラが捉えた画像です

60
00:04:21,962 --> 00:04:25,465
その画像を
ARにレンダリングします

61
00:04:25,732 --> 00:04:28,135
追跡した動きも同様です

62
00:04:28,702 --> 00:04:31,538
仮想カメラに反映されます

63
00:04:31,638 --> 00:04:36,376
カメラが捉えた情報と
同じ位置関係で

64
00:04:36,476 --> 00:04:38,078
レンダリングされます

65
00:04:38,679 --> 00:04:44,017
検知した平面の情報など
環境の情報も含まれます

66
00:04:45,619 --> 00:04:49,556
では 初期の追跡技術の
話を始めましょう

67
00:04:51,525 --> 00:04:53,493
位置特定についてです

68
00:04:54,361 --> 00:04:56,997
位置特定とは何でしょう？

69
00:04:57,097 --> 00:05:01,535
回転だけを追跡することを
表しています

70
00:04:57,097 --> 00:05:01,535
回転だけを追跡することを
表しています

71
00:05:02,069 --> 00:05:04,771
つまり 首を動かすだけで

72
00:05:04,872 --> 00:05:09,276
仮想コンテンツを
追跡するということです

73
00:05:09,877 --> 00:05:15,015
この場合 同じ位置の
仮想コンテンツは取得できても

74
00:05:15,148 --> 00:05:18,385
違う位置からのものは
できません

75
00:05:19,520 --> 00:05:22,923
３つの軸の動きだけを
追跡します

76
00:05:23,023 --> 00:05:26,493
これが 3DOFと
呼ばれる理由です

77
00:05:27,127 --> 00:05:30,631
球状の仮想環境で
活用できます

78
00:05:30,731 --> 00:05:34,401
例えば 360度動画です

79
00:05:34,501 --> 00:05:38,505
仮想コンテンツを
同じ位置から見られます

80
00:05:39,273 --> 00:05:43,544
遠くにある物を
拡大するためにも使えます

81
00:05:44,311 --> 00:05:48,916
さまざまな視点で
物を見たい場合には

82
00:05:49,016 --> 00:05:53,187
位置特定による拡大は
適切ではありません

83
00:05:54,988 --> 00:05:59,760
位置特定を実行中の
内部の処理を説明します

84
00:06:01,161 --> 00:06:02,963
極めてシンプルです

85
00:06:03,130 --> 00:06:06,433
回転データだけを使用して

86
00:06:06,533 --> 00:06:10,337
モーションセンサーの
データに適用します

87
00:06:11,338 --> 00:06:16,410
動作データは カメラの
画像よりも早く提供されます

88
00:06:16,577 --> 00:06:19,580
そのため 画像が
有効になってから

89
00:06:19,680 --> 00:06:23,584
最新の動作データを
取得します

90
00:06:23,684 --> 00:06:27,321
そして 双方の結果を
返します

91
00:06:27,421 --> 00:06:29,256
これだけです

92
00:06:29,590 --> 00:06:34,661
カメラフィードは
位置特定で処理されません

93
00:06:34,761 --> 00:06:37,397
コンピュータビジョンが
ないのです

94
00:06:38,232 --> 00:06:42,636
位置特定処理を
実行するためには

95
00:06:42,736 --> 00:06:47,508
ARSessionを
構成する必要があります

96
00:06:48,208 --> 00:06:52,779
処理結果は
ARFrameから提供された―

97
00:06:53,580 --> 00:06:55,782
ARCameraに返ります

98
00:06:56,316 --> 00:06:59,319
ARCameraのtransformには

99
00:06:59,419 --> 00:07:04,057
位置特定においては
回転データのみが含まれます

100
00:06:59,419 --> 00:07:04,057
位置特定においては
回転データのみが含まれます

101
00:07:04,158 --> 00:07:06,393
カメラが追跡したデータです

102
00:07:07,060 --> 00:07:12,032
eulerAnglesでも
代用が可能です

103
00:07:12,199 --> 00:07:14,601
最適なほうを使ってください

104
00:07:16,803 --> 00:07:20,841
次は より高度な
追跡技術の話をしましょう

105
00:07:21,074 --> 00:07:22,843
方向特定についてです

106
00:07:24,545 --> 00:07:28,582
方向特定は
カメラビューの位置や

107
00:07:28,682 --> 00:07:32,152
位置の変更を追跡し
現実世界に反映

108
00:07:32,252 --> 00:07:35,622
現実世界に関する
事前の情報は不要です

109
00:07:36,190 --> 00:07:38,425
では 左をご覧ください

110
00:07:39,493 --> 00:07:42,229
実際のカメラビューの
映像です

111
00:07:42,329 --> 00:07:46,366
右は カメラの動きを
追跡した映像です

112
00:07:46,466 --> 00:07:51,972
座標系で表示された
環境を探索しています

113
00:07:52,906 --> 00:07:56,643
何が起きているか
詳しく説明します

114
00:07:58,145 --> 00:08:01,415
方向特定では
モーションセンサーと

115
00:07:58,145 --> 00:08:01,415
方向特定では
モーションセンサーと

116
00:08:02,082 --> 00:08:06,520
加速度計が測位した
動作データを使います

117
00:08:06,854 --> 00:08:12,726
方向や移動の変化を
頻繁に演算するためです

118
00:08:14,962 --> 00:08:19,600
また Metalの情報も
提供します

119
00:08:20,901 --> 00:08:26,273
追跡システムのこの技術は
慣性オドメトリと呼ばれます

120
00:08:26,974 --> 00:08:30,477
この動作データは
優れた動作情報を

121
00:08:30,577 --> 00:08:33,347
短い間隔で提供します

122
00:08:33,447 --> 00:08:38,818
ただし 突然の動きがあると
間隔は長くなります

123
00:08:38,919 --> 00:08:43,756
データの精度が不十分で
誤差と仮定されるからです

124
00:08:44,424 --> 00:08:47,761
このため
単独で使用できません

125
00:08:47,995 --> 00:08:50,230
では 誤差を補正します

126
00:08:51,632 --> 00:08:56,603
方向特定では
コンピュータビジョンを適用

127
00:08:57,070 --> 00:09:00,174
そして カメラフレームを
使います

128
00:08:57,070 --> 00:09:00,174
そして カメラフレームを
使います

129
00:09:01,074 --> 00:09:06,747
この技術の精度は高いですが
演算時間が長くかかります

130
00:09:08,081 --> 00:09:12,553
また カメラの動きに対する
感度が高いので

131
00:09:12,653 --> 00:09:15,856
被写体がぶれる
原因にもなります

132
00:09:16,490 --> 00:09:21,828
この技術は
視覚オドメトリと呼ばれます

133
00:09:21,929 --> 00:09:27,234
コンピュータビジョンと
モーションデータの融合で

134
00:09:27,334 --> 00:09:30,470
ARKitは双方の
利点を持ちます

135
00:09:30,571 --> 00:09:35,475
コンピュータビジョンの
利点は精度の高さです

136
00:09:35,709 --> 00:09:39,346
モーションデータの利点は
更新速度と

137
00:09:39,446 --> 00:09:44,551
短い間隔でも
正確な情報を提供する点です

138
00:09:46,086 --> 00:09:51,058
２つが融合すると
コンピュータビジョンを省けます

139
00:09:51,158 --> 00:09:56,263
一方で 効率的で感度の高い
追跡を維持できるのです

140
00:09:56,864 --> 00:10:01,969
CPUが解放されるので
アプリケーションに使えます

141
00:09:56,864 --> 00:10:01,969
CPUが解放されるので
アプリケーションに使えます

142
00:10:02,803 --> 00:10:08,041
２つを融合した技術を
視覚慣性オドメトリと呼びます

143
00:10:08,842 --> 00:10:13,046
視覚的なことを
詳しく見ていきましょう

144
00:10:14,414 --> 00:10:18,919
コンピュータビジョンの
処理において

145
00:10:19,019 --> 00:10:23,991
興味深いものを抜粋したのが
青とオレンジの点です

146
00:10:24,391 --> 00:10:27,761
この２つの点は
同じ環境下で

147
00:10:27,861 --> 00:10:31,965
画像が異なる場合も
確実に抜粋されます

148
00:10:32,933 --> 00:10:35,802
フィーチャーと
呼ばれる領域です

149
00:10:36,537 --> 00:10:41,742
フィーチャーは類似性と
外見を基にして

150
00:10:41,842 --> 00:10:44,912
さまざまな画像と一致します

151
00:10:45,145 --> 00:10:49,449
すると 3Dとして
見える状態になります

152
00:10:50,083 --> 00:10:54,788
２つのフィーチャーが
両側に並んでいます

153
00:10:54,988 --> 00:10:58,225
この２点間の
視差は重要です

154
00:10:58,325 --> 00:11:02,429
全く異なる視点を
もたらすことで

155
00:10:58,325 --> 00:11:02,429
全く異なる視点を
もたらすことで

156
00:11:02,529 --> 00:11:06,700
立体的に見え
深度を認識できるからです

157
00:11:07,067 --> 00:11:09,303
これがARKitの現状です

158
00:11:09,403 --> 00:11:14,474
同じカメラで
さまざまな視点を処理します

159
00:11:14,675 --> 00:11:18,545
機能するのは
十分な視差がある場合です

160
00:11:18,846 --> 00:11:23,517
ARKitは 足りない
深度の情報も演算します

161
00:11:23,617 --> 00:11:26,286
つまり
2Dのフィーチャーが

162
00:11:26,520 --> 00:11:29,590
3Dに再構築されるのです

163
00:11:30,724 --> 00:11:34,828
ただし 再構築を
成功させるためには

164
00:11:35,629 --> 00:11:39,466
カメラの位置を
変える必要があります

165
00:11:39,566 --> 00:11:41,902
十分な視差を得るためです

166
00:11:42,269 --> 00:11:44,771
例えば 横への移動です

167
00:11:44,872 --> 00:11:49,209
回転するだけでは
十分な情報は得られません

168
00:11:50,444 --> 00:11:53,480
つまり これが
最初の地図です

169
00:11:53,580 --> 00:11:56,450
ARKitでは
World Mapと呼びます

170
00:11:57,317 --> 00:12:03,090
同時に シーケンスの
カメラ位置と方向を演算

171
00:11:57,317 --> 00:12:03,090
同時に シーケンスの
カメラ位置と方向を演算

172
00:12:03,223 --> 00:12:07,127
Ｃの文字が表示されます

173
00:12:07,394 --> 00:12:09,296
追跡システムである―

174
00:12:09,396 --> 00:12:13,333
方向特定の
初期化を意味します

175
00:12:14,501 --> 00:12:19,306
World Mapが
再構築されると同時に

176
00:12:19,406 --> 00:12:21,708
起点も明示されます

177
00:12:22,009 --> 00:12:27,881
カメラの最初の起点に
設定されるのです

178
00:12:27,981 --> 00:12:30,717
また 重力も設定されます

179
00:12:30,984 --> 00:12:33,420
Ｗの文字が表示されます

180
00:12:34,788 --> 00:12:38,125
これで 小さな
仮想世界ができました

181
00:12:38,225 --> 00:12:42,029
World Mapとして
再構築されています

182
00:12:42,863 --> 00:12:48,302
また カメラは現実世界の
動きを追跡してくれます

183
00:12:51,138 --> 00:12:57,411
では 仮想コンテンツを
カメラビューに付与しましょう

184
00:12:58,579 --> 00:13:03,116
ARSessionに
正確な情報を付与するには

185
00:12:58,579 --> 00:13:03,116
ARSessionに
正確な情報を付与するには

186
00:13:03,217 --> 00:13:06,453
ARAnchorを
使うとよいでしょう

187
00:13:06,553 --> 00:13:08,455
Ａの文字が表示されます

188
00:13:09,556 --> 00:13:13,660
ARAnchorは
World Mapにおける

189
00:13:13,760 --> 00:13:16,063
基準点です

190
00:13:16,463 --> 00:13:19,500
方向特定は
更新されるので

191
00:13:19,600 --> 00:13:22,035
ARAnchorを使うべきです

192
00:13:22,135 --> 00:13:26,440
つまり 仮想コンテンツも
更新され 正しい情報が―

193
00:13:27,808 --> 00:13:30,978
カメラビューに
付与されます

194
00:13:32,679 --> 00:13:37,951
ARAnchorで 仮想コンテンツを
付与できるのです

195
00:13:38,118 --> 00:13:44,458
ARAnchorが情報を正しく
カメラビューに付与します

196
00:13:46,093 --> 00:13:50,697
これで 3Dの
World Mapを作りました

197
00:13:50,797 --> 00:13:53,500
方向特定の参照システムです

198
00:13:53,967 --> 00:13:56,937
新しい画像の参照に用います

199
00:13:57,070 --> 00:14:02,242
フィーチャーが一致すると
三角測量を行います

200
00:13:57,070 --> 00:14:02,242
フィーチャーが一致すると
三角測量を行います

201
00:14:02,609 --> 00:14:07,881
同時に 新しいフィーチャーの
抽出 一致 三角測量が行われ

202
00:14:07,981 --> 00:14:11,084
World Mapに付与されます

203
00:14:11,185 --> 00:14:14,521
つまり ARKitが
学習するのです

204
00:14:15,856 --> 00:14:18,759
学習により
追跡のための演算が

205
00:14:18,859 --> 00:14:22,296
最新のカメラ位置と
方向を更新します

206
00:14:22,996 --> 00:14:27,734
そして最新の情報が
カメラビューに反映されます

207
00:14:30,103 --> 00:14:32,639
周囲を探索している間も

208
00:14:32,773 --> 00:14:36,376
方向特定は
カメラの動きを追跡します

209
00:14:36,477 --> 00:14:39,880
現実世界の学習を
続けるのです

210
00:14:40,414 --> 00:14:44,484
しかし 徐々に
ARにずれが生じます

211
00:14:44,818 --> 00:14:48,355
例えば 左のような状態です

212
00:14:48,455 --> 00:14:51,325
わずかな偏差がありますね

213
00:14:52,292 --> 00:14:57,664
たとえ わずかな偏差や
誤差であっても

214
00:14:57,764 --> 00:15:01,702
蓄積されれば
目立つようになります

215
00:14:57,764 --> 00:15:01,702
蓄積されれば
目立つようになります

216
00:15:03,403 --> 00:15:06,673
端末が
過去に探索した場所に

217
00:15:06,773 --> 00:15:09,743
戻ってきた場合の
説明をします

218
00:15:09,843 --> 00:15:12,846
例えば 探索の
開始地点に戻ると

219
00:15:12,946 --> 00:15:16,149
別の最適化処理の
実行が可能です

220
00:15:16,517 --> 00:15:20,521
視覚慣性オドメトリが

221
00:15:20,654 --> 00:15:26,393
視覚慣性SLAMシステムを
作ります

222
00:15:27,528 --> 00:15:32,799
では 方向特定が探索を
開始した画像に戻りましょう

223
00:15:33,867 --> 00:15:39,273
過去と現在に見た
風景のWorld Mapと

224
00:15:39,373 --> 00:15:43,744
追跡情報の
一致具合を確認します

225
00:15:43,844 --> 00:15:46,280
例えば 最初の風景と比較

226
00:15:47,948 --> 00:15:51,285
そして 最適化を実行します

227
00:15:51,985 --> 00:15:55,456
現在の情報とWorld Mapを

228
00:15:55,556 --> 00:15:58,091
現実世界に調和させます

229
00:15:59,059 --> 00:16:02,830
ARAnchorも
更新されましたね

230
00:15:59,059 --> 00:16:02,830
ARAnchorも
更新されましたね

231
00:16:02,930 --> 00:16:06,233
これが 仮想コンテンツを
付与する際に

232
00:16:06,333 --> 00:16:09,770
ARAnchorを
使うべき理由です

233
00:16:12,873 --> 00:16:17,711
この流れをカメラフィードの
目線で見てみましょう

234
00:16:18,045 --> 00:16:21,448
左が カメラフィードの目線です

235
00:16:21,548 --> 00:16:25,219
画像を追跡する
フィーチャーも見えます

236
00:16:25,319 --> 00:16:29,156
右に映るのは
風景の鳥かん図です

237
00:16:29,256 --> 00:16:32,726
ARKitが風景を認識し
3Dに―

238
00:16:33,927 --> 00:16:36,797
再構築しているのが
分かります

239
00:16:38,098 --> 00:16:42,736
再構築する場所を
符号化したのが点です

240
00:16:42,836 --> 00:16:47,341
青が床 赤がテーブルと
イスを表しています

241
00:16:49,409 --> 00:16:55,148
カメラが 出発地点などの
過去に見た場所に戻ると

242
00:16:55,249 --> 00:16:58,352
ARKitは
最適化を実行します

243
00:16:58,452 --> 00:17:01,755
点とカメラの軌道を
見てください

244
00:16:58,452 --> 00:17:01,755
点とカメラの軌道を
見てください

245
00:17:02,756 --> 00:17:05,759
更新に気が付きましたか？

246
00:17:08,529 --> 00:17:13,166
この更新でARKitと
現実世界の調整をします

247
00:17:13,267 --> 00:17:16,103
カメラの動きも調整します

248
00:17:16,203 --> 00:17:21,141
これで カメラフレームの
精度が高くなるのです

249
00:17:21,875 --> 00:17:27,214
ところで 方向特定が
演算や学習した情報は全て

250
00:17:27,314 --> 00:17:31,985
端末だけで
処理を実行しています

251
00:17:32,085 --> 00:17:35,389
また 保存先も端末だけです

252
00:17:36,990 --> 00:17:40,961
では アプリケーションで
どう使うのでしょう

253
00:17:41,862 --> 00:17:43,530
答えは簡単です

254
00:17:45,666 --> 00:17:49,236
方向特定を稼働するためには

255
00:17:49,336 --> 00:17:52,172
ARSessionを構成します

256
00:17:52,906 --> 00:17:58,178
結果は ARFrameの
ARCameraに返ります

257
00:18:01,148 --> 00:18:05,018
ARCameraは
方向特定の場合

258
00:18:05,119 --> 00:18:07,287
transformを含みます

259
00:18:07,421 --> 00:18:12,459
回転や移動で
カメラの動きを追跡できます

260
00:18:13,193 --> 00:18:18,031
ARCameraに含まれるのは
追跡の情報と状態や

261
00:18:18,131 --> 00:18:20,134
trackingStateReasonです

262
00:18:21,368 --> 00:18:25,706
このインスタンスが
追跡の品質を提供します

263
00:18:26,607 --> 00:18:28,275
追跡の品質です

264
00:18:28,675 --> 00:18:31,745
ARのアプリケーションを
使ったら

265
00:18:31,845 --> 00:18:35,983
追跡が機能しなかった
経験はありますか？

266
00:18:36,250 --> 00:18:37,851
どう感じましたか？

267
00:18:38,385 --> 00:18:41,488
イライラして
再起動した？

268
00:18:42,256 --> 00:18:45,792
どうすれば
追跡の品質は向上するのか？

269
00:18:46,627 --> 00:18:51,899
品質に影響を与える原因を
理解する必要があります

270
00:18:52,065 --> 00:18:54,167
主な原因は３つです

271
00:18:55,002 --> 00:18:58,672
１つめは
方向特定が 画像と

272
00:18:58,772 --> 00:19:01,375
センサーデータに
依存すること

273
00:18:58,772 --> 00:19:01,375
センサーデータに
依存すること

274
00:19:01,475 --> 00:19:05,979
長時間 妨害があると
追跡は制限されます

275
00:19:06,813 --> 00:19:11,852
２つめは 方向特定が
機能する環境の問題です

276
00:19:11,952 --> 00:19:15,989
方向特定は 視覚的に
安定した位置を使い

277
00:19:16,089 --> 00:19:18,692
三角測量を行うからです

278
00:19:18,792 --> 00:19:24,097
視覚的複雑性が
十分にあることが重要です

279
00:19:24,598 --> 00:19:30,037
例えば 環境が暗すぎたり
背景が白い壁だったりすると

280
00:19:30,137 --> 00:19:33,006
追跡の品質は落ちます

281
00:19:34,208 --> 00:19:35,275
３つめです

282
00:19:35,776 --> 00:19:39,913
方向特定は 静的な環境で
最も機能します

283
00:19:40,247 --> 00:19:43,584
カメラが動きを捉えすぎると

284
00:19:44,351 --> 00:19:48,355
画像とモーションデータは
一致しません

285
00:19:49,122 --> 00:19:52,426
これが ずれの
原因になります

286
00:19:52,859 --> 00:19:57,798
バスなどの動く場所では
使わないほうがいいです

287
00:19:58,232 --> 00:20:03,904
視覚的な変化はなくても
モーションセンサーが

288
00:19:58,232 --> 00:20:03,904
視覚的な変化はなくても
モーションセンサーが

289
00:20:04,171 --> 00:20:07,975
上下の動きを
検知してしまうからです

290
00:20:09,676 --> 00:20:13,547
では ユーザが使用中の
追跡の品質は

291
00:20:13,647 --> 00:20:16,783
どうやって分かるでしょう

292
00:20:18,185 --> 00:20:20,921
ARKitの監視で分かります

293
00:20:21,088 --> 00:20:25,592
認識しづらい状況での
情報を集めたデータで

294
00:20:25,692 --> 00:20:30,764
訓練を行った
機械学習を適用しました

295
00:20:31,698 --> 00:20:36,270
追跡の性能を伝える
分類子を訓練するために

296
00:20:36,370 --> 00:20:41,475
目に見える
アノテーションを使いました

297
00:20:41,842 --> 00:20:44,845
端末の速度も使っています

298
00:20:45,412 --> 00:20:48,782
稼働時の 追跡の健全性は

299
00:20:48,882 --> 00:20:52,252
パラメータに基づいて
決まります

300
00:20:52,553 --> 00:20:56,457
健全性の評価は
このビデオの

301
00:20:56,557 --> 00:20:59,626
左下に表示されています

302
00:20:59,993 --> 00:21:02,596
カメラが覆われると―

303
00:20:59,993 --> 00:21:02,596
カメラが覆われると―

304
00:21:03,497 --> 00:21:06,700
動いていても
健全性が悪化

305
00:21:07,701 --> 00:21:12,940
覆いが外れると
健全性は元に戻ります

306
00:21:13,941 --> 00:21:17,511
ARKitが 追跡の状態を提供し

307
00:21:17,811 --> 00:21:20,114
情報を簡易化します

308
00:21:20,447 --> 00:21:25,385
追跡の状態は３つあります
１つは普通の状態です

309
00:21:25,752 --> 00:21:29,289
健全な状態を意味します

310
00:21:29,389 --> 00:21:32,392
大体の場合は
健全な状態です

311
00:21:32,493 --> 00:21:36,697
２つめは 性能が悪く
制限がある状態

312
00:21:37,431 --> 00:21:38,899
この場合は―

313
00:21:39,833 --> 00:21:43,337
制限がかかる理由があります

314
00:21:43,437 --> 00:21:46,773
例えば フィーチャーが
不十分だとか

315
00:21:46,874 --> 00:21:50,477
初期化の段階にあるためです

316
00:21:51,879 --> 00:21:56,550
３つめは 追跡が
始まっていない無効の状態

317
00:21:57,050 --> 00:22:01,255
カメラの追跡状態が
変わる度に

318
00:21:57,050 --> 00:22:01,255
カメラの追跡状態が
変わる度に

319
00:22:01,355 --> 00:22:04,358
デリゲートが
呼び出されます

320
00:22:05,159 --> 00:22:08,729
そのため 制限が
かかった時には

321
00:22:08,829 --> 00:22:11,465
ユーザに通知されます

322
00:22:12,032 --> 00:22:16,336
そこで ユーザが
追跡状態を改善するのに

323
00:22:16,437 --> 00:22:19,907
有効なフィードバックを
提供しますので

324
00:22:20,073 --> 00:22:23,577
大体は ユーザ自身で
改善できます

325
00:22:23,677 --> 00:22:27,915
例えば 先程学んだように
横へ動いて

326
00:22:28,015 --> 00:22:30,184
初期化を行うこと

327
00:22:30,350 --> 00:22:33,687
または 視覚的複雑性を
得るため

328
00:22:33,787 --> 00:22:36,223
光を確保することです

329
00:22:37,758 --> 00:22:40,761
方向特定について
まとめましょう

330
00:22:44,464 --> 00:22:50,437
方向特定は カメラを
6DOFで追跡する技術です

331
00:22:51,338 --> 00:22:54,274
ユーザの周辺の環境を追跡し

332
00:22:54,374 --> 00:22:57,945
その環境に関する
事前情報は不要です

333
00:22:58,111 --> 00:23:01,648
そして どの視点でも
見ることができる情報を

334
00:22:58,111 --> 00:23:01,648
そして どの視点でも
見ることができる情報を

335
00:23:01,748 --> 00:23:05,419
現実世界に付与します

336
00:23:06,553 --> 00:23:09,723
またWorld Mapも作成

337
00:23:09,823 --> 00:23:12,793
これは 新しい画像を
特定する―

338
00:23:12,893 --> 00:23:15,796
追跡の
参照システムとなります

339
00:23:17,197 --> 00:23:21,735
すばらしい体験を創るため
追跡の性能を監視し

340
00:23:21,869 --> 00:23:25,539
ユーザに
フィードバックも提供します

341
00:23:26,907 --> 00:23:32,279
ユーザの端末だけで
処理 保存されます

342
00:23:33,380 --> 00:23:37,751
デベロッパ用の
サンプルを試してください

343
00:23:37,851 --> 00:23:40,821
“Building Your First
AR Experience”です

344
00:23:40,921 --> 00:23:43,390
さまざまな状況で

345
00:23:43,490 --> 00:23:48,428
追跡の性能を
試してみてくださいね

346
00:23:48,529 --> 00:23:50,998
ユーザに伝えてください

347
00:23:51,098 --> 00:23:56,303
追跡しづらい状況に
陥ったとしても

348
00:23:56,537 --> 00:24:00,541
すばらしい性能を
保証しています

349
00:23:56,537 --> 00:24:00,541
すばらしい性能を
保証しています

350
00:24:02,709 --> 00:24:05,312
方向特定とは カメラが

351
00:24:05,412 --> 00:24:09,650
現実世界を映している
場所のことでした

352
00:24:10,083 --> 00:24:11,385
次に進みます

353
00:24:11,485 --> 00:24:16,957
仮想コンテンツと現実世界の
関わり方の話をしましょう

354
00:24:17,057 --> 00:24:19,993
平面検出についてです

355
00:24:23,163 --> 00:24:28,135
平面検出を使用した例を
ビデオでお見せします

356
00:24:28,402 --> 00:24:34,341
仮想コンテンツを
配置し相互作用を体験します

357
00:24:35,542 --> 00:24:41,615
初めに アプリケーションが
ユーザの動きを促します

358
00:24:42,216 --> 00:24:45,452
水平面を検出すると

359
00:24:45,552 --> 00:24:50,257
配置できる仮想のテーブルが
表示されます

360
00:24:51,158 --> 00:24:55,929
配置したら 好きな方向に
回転させ 固定します

361
00:24:56,096 --> 00:25:02,035
地面とテーブルを固定した時
相互作用に気付きましたか？

362
00:24:56,096 --> 00:25:02,035
地面とテーブルを固定した時
相互作用に気付きましたか？

363
00:25:02,135 --> 00:25:05,139
一瞬ですが はずんだのは

364
00:25:05,239 --> 00:25:09,142
平面がどこなのか
認識できているからです

365
00:25:09,476 --> 00:25:12,446
では 内部の動きを
見ましょう

366
00:25:14,248 --> 00:25:16,850
World Mapを使います

367
00:25:16,950 --> 00:25:22,022
World Mapのことは
先程ご説明しました

368
00:25:22,289 --> 00:25:25,259
画面には
黄色で表示しています

369
00:25:26,093 --> 00:25:28,996
World Mapが検知するのは

370
00:25:29,096 --> 00:25:31,532
水平か垂直な表面です

371
00:25:31,632 --> 00:25:34,868
例えば 地面や
ベンチ 壁です

372
00:25:35,302 --> 00:25:39,706
情報を蓄積することで
検出が可能です

373
00:25:40,007 --> 00:25:42,910
ユーザが動き回るほど

374
00:25:43,010 --> 00:25:46,613
より多くの表面の
情報が取得されます

375
00:25:46,813 --> 00:25:49,116
平面検出を使えば

376
00:25:49,216 --> 00:25:53,253
表面が広がった
凸包なども検出できます

377
00:25:55,989 --> 00:26:01,228
表面に 複数の平面が
検出された場合の話をします

378
00:25:55,989 --> 00:26:01,228
表面に 複数の平面が
検出された場合の話をします

379
00:26:01,328 --> 00:26:04,965
画面の 緑色と
紫色の部分のことです

380
00:26:05,165 --> 00:26:08,268
重複すると
融合を始めます

381
00:26:09,736 --> 00:26:12,606
水平と垂直の面が交差すると

382
00:26:12,706 --> 00:26:17,811
２つの面が切られるのが
ARKit 2の特徴です

383
00:26:20,114 --> 00:26:23,951
平面検出は 負荷の低い
設計をしています

384
00:26:24,351 --> 00:26:28,589
方向特定で作った点を
転用するからです

385
00:26:28,989 --> 00:26:32,292
平面検出が点と一致すると

386
00:26:32,392 --> 00:26:36,964
少しずつ 継続的に
多くの点を集めていきます

387
00:26:37,064 --> 00:26:40,100
そして 重複すると
融合するのです

388
00:26:41,168 --> 00:26:45,272
なので 最初の検出は
時間がかかります

389
00:26:46,039 --> 00:26:47,541
どういうことか？

390
00:26:48,776 --> 00:26:52,546
アプリケーションを
起動しても すぐに

391
00:26:52,646 --> 00:26:56,250
平面は検出されない
ということです

392
00:26:57,184 --> 00:27:01,588
平面の検出が
強制である場合は

393
00:26:57,184 --> 00:27:01,588
平面の検出が
強制である場合は

394
00:27:01,688 --> 00:27:04,258
ユーザを導いてください

395
00:27:04,358 --> 00:27:08,262
カメラを移動させて
視差を基に

396
00:27:08,362 --> 00:27:12,332
再構築できるように
するのです

397
00:27:12,566 --> 00:27:15,369
視覚的複雑性も
得てください

398
00:27:15,969 --> 00:27:20,874
再構成する場合
回転だけでは不十分です

399
00:27:22,709 --> 00:27:25,145
平面検出を
有効にするには？

400
00:27:25,712 --> 00:27:27,147
とても簡単です

401
00:27:27,247 --> 00:27:31,251
方向特定の
3Dマップを再利用するので

402
00:27:31,351 --> 00:27:35,389
クラスを使って構築できます

403
00:27:36,623 --> 00:27:39,893
次は 平面検出の
属性の設定です

404
00:27:39,993 --> 00:27:44,431
水平か垂直
どちらかを設定しましょう

405
00:27:45,032 --> 00:27:50,270
そして ARSessionを
呼び出します

406
00:27:50,370 --> 00:27:53,273
すると 平面の検出を
開始します

407
00:27:54,474 --> 00:27:58,278
結果は どのように
返るのでしょうか

408
00:28:01,748 --> 00:28:05,352
ARPlaneAnchorに
結果が返ります

409
00:28:05,853 --> 00:28:09,289
ARAnchorのサブクラスです

410
00:28:10,023 --> 00:28:12,860
“transform”を提供します

411
00:28:13,494 --> 00:28:17,898
Anchorの位置情報を
持っています

412
00:28:19,166 --> 00:28:22,369
ARPlaneAnchorも

413
00:28:22,469 --> 00:28:26,173
平面の配列の
情報を持っています

414
00:28:26,740 --> 00:28:29,142
２つの方法で
表示されます

415
00:28:29,243 --> 00:28:33,580
１つは 境界ボックスとして
表示する方法

416
00:28:34,248 --> 00:28:38,418
または 検出した
平面の凸包を

417
00:28:38,519 --> 00:28:41,655
3Dメッシュとして
表示する方法です

418
00:28:42,556 --> 00:28:45,826
平面の検知の通知を
得たい場合は―

419
00:28:47,327 --> 00:28:49,463
デリゲートを呼び出します

420
00:28:49,997 --> 00:28:54,268
追加 更新 削除が
あった場合に呼び出します

421
00:28:54,568 --> 00:28:57,971
これで 変更した平面を
使えますし

422
00:28:58,071 --> 00:29:00,474
更新にも反応できます

423
00:28:58,071 --> 00:29:00,474
更新にも反応できます

424
00:29:01,575 --> 00:29:03,911
平面で何ができるのか？

425
00:29:04,778 --> 00:29:07,848
先程見た映像がいい例です

426
00:29:07,948 --> 00:29:11,218
例えば 仮想コンテンツを
配置したり

427
00:29:11,985 --> 00:29:15,122
物理的な
相互作用ができます

428
00:29:15,222 --> 00:29:19,026
先程のように
はずむこともできますね

429
00:29:19,760 --> 00:29:24,464
オクルージョンを追加して
使うこともできます

430
00:29:24,565 --> 00:29:29,069
追加した平面の後ろにある
仮想コンテンツを

431
00:29:29,169 --> 00:29:31,171
隠すことができるのです

432
00:29:32,840 --> 00:29:35,709
では 内容を
整理しましょう

433
00:29:36,477 --> 00:29:39,580
まず 位置特定の
話をしました

434
00:29:40,581 --> 00:29:42,149
次に 方向特定

435
00:29:42,950 --> 00:29:44,651
そして 平面検出です

436
00:29:45,219 --> 00:29:49,890
この後 ミケーレが
新しい追跡技術を紹介します

437
00:29:49,990 --> 00:29:52,226
ARKit 2です

438
00:29:52,593 --> 00:29:54,027
ミケーレを呼びましょう

439
00:29:54,328 --> 00:29:57,831
(拍手)

440
00:29:58,332 --> 00:29:59,500
ありがとう

441
00:30:00,434 --> 00:30:04,671
私はミケーレです
後半を担当できて光栄です

442
00:30:05,038 --> 00:30:08,642
World Mapの保存と
ロードについて話します

443
00:30:08,876 --> 00:30:13,213
取得した情報を全て
保存する機能です

444
00:30:13,313 --> 00:30:17,518
つまり 後で情報を
ロードして使っても

445
00:30:17,618 --> 00:30:21,922
ARの体験に一貫性を
持たせることができます

446
00:30:22,322 --> 00:30:28,295
別の端末でロードして
複数での利用も可能です

447
00:30:28,562 --> 00:30:30,230
例を見てみましょう

448
00:30:37,004 --> 00:30:40,541
映像に映る男性は
アンドレです

449
00:30:40,641 --> 00:30:44,812
テーブルの周りを歩き
ARを体験しています

450
00:30:45,312 --> 00:30:47,815
彼のカメラの映像が
これです

451
00:30:47,915 --> 00:30:52,152
仮想の花瓶をテーブルの上に
置いています

452
00:30:54,822 --> 00:30:58,258
数分後 彼の友人が
やって来ました

453
00:30:58,358 --> 00:31:00,460
同じ風景を見ています

454
00:30:58,358 --> 00:31:00,460
同じ風景を見ています

455
00:31:00,561 --> 00:31:05,966
左がアンドレの画面
右が彼の友人の画面です

456
00:31:06,467 --> 00:31:10,070
同じ風景を
見ているのが分かります

457
00:31:10,170 --> 00:31:13,807
重要なのは 同じものが
見えていることです

458
00:31:14,208 --> 00:31:17,077
AR体験を
共有しているのです

459
00:31:19,146 --> 00:31:24,084
この例は ３段階に
分けることができます

460
00:31:24,184 --> 00:31:28,055
初めに アンドレが
World Mapを取得

461
00:31:28,856 --> 00:31:31,859
次に World Mapを
共有します

462
00:31:32,826 --> 00:31:36,363
最後に 友人の端末に
対応させます

463
00:31:37,431 --> 00:31:43,237
つまり 新しい端末でも
同じ位置だと認識できるのです

464
00:31:43,504 --> 00:31:47,207
World Mapの
正確な位置を演算し

465
00:31:47,307 --> 00:31:52,212
違う端末でも 同じ位置から
追跡を始めるのです

466
00:31:53,614 --> 00:31:56,650
詳しい説明をする前に

467
00:31:56,783 --> 00:32:00,187
World Mapを
復習しましょう

468
00:31:56,783 --> 00:32:00,187
World Mapを
復習しましょう

469
00:32:01,388 --> 00:32:06,793
World Mapは
追跡データを保持しています

470
00:32:06,894 --> 00:32:10,697
例えば 先程の
フィーチャーポイントや

471
00:32:10,797 --> 00:32:13,133
ポイントの見え方の情報です

472
00:32:16,970 --> 00:32:20,607
システムが追加した
平面などの

473
00:32:20,707 --> 00:32:23,977
Anchorも保持しています

474
00:32:24,812 --> 00:32:29,883
または ユーザが追加した
花瓶などの情報です

475
00:32:31,318 --> 00:32:34,822
このデータは
シリアル変換が可能なので

476
00:32:34,922 --> 00:32:40,127
複数のユーザが
AR体験を共有できます

477
00:32:40,227 --> 00:32:44,398
では World Mapの
取得の説明です

478
00:32:46,066 --> 00:32:50,070
最初のビデオを
もう一度流します

479
00:32:50,237 --> 00:32:52,940
左がアンドレの画面です

480
00:32:55,742 --> 00:33:00,581
右には World Mapが
映っています

481
00:32:55,742 --> 00:33:00,581
右には World Mapが
映っています

482
00:33:00,681 --> 00:33:04,418
テーブルとイスが
見えています

483
00:33:05,385 --> 00:33:09,623
取得する際に
注意すべき点があります

484
00:33:09,957 --> 00:33:14,261
マリオンが話した注意点が
全て当てはまります

485
00:33:14,361 --> 00:33:19,032
つまり 高密度の
フィーチャーが必要です

486
00:33:19,466 --> 00:33:21,568
静的な風景も必要です

487
00:33:22,002 --> 00:33:25,839
わずかな変化には
対処できますが

488
00:33:25,939 --> 00:33:28,342
大部分は静的が
いいでしょう

489
00:33:28,942 --> 00:33:32,913
さらに World Mapを
共有する場合

490
00:33:33,013 --> 00:33:36,950
複数の視点から
環境を見ます

491
00:33:37,451 --> 00:33:43,123
特に 後で対応させる場所の
情報は全て取得しましょう

492
00:33:45,292 --> 00:33:50,097
World mapping statusを
用意しました

493
00:33:50,197 --> 00:33:52,900
これで簡単に取得できます

494
00:33:53,801 --> 00:33:56,336
これまでに説明した内容を

495
00:33:56,670 --> 00:33:59,273
要約してお伝えします

496
00:33:59,506 --> 00:34:03,110
最初は World Mapに
制限があります

497
00:33:59,506 --> 00:34:03,110
最初は World Mapに
制限があります

498
00:34:03,210 --> 00:34:07,948
風景を学習するにつれて
徐々に有効になります

499
00:34:08,047 --> 00:34:13,253
最後は World Mapが
作成済みになります

500
00:34:13,654 --> 00:34:17,424
そのあと World Mapを
保存します

501
00:34:18,992 --> 00:34:21,161
これは よい情報ですが

502
00:34:21,261 --> 00:34:24,764
ユーザに
適用される情報です

503
00:34:24,864 --> 00:34:29,436
デベロッパは ユーザを
導く必要があります

504
00:34:30,204 --> 00:34:34,475
状態を表示して
作成済みになるまでは

505
00:34:34,574 --> 00:34:39,947
World Mapの保存や
共有を無効にできるのです

506
00:34:42,049 --> 00:34:47,021
また 情報の取得中に
追跡の性能を監視できます

507
00:34:47,487 --> 00:34:52,592
追跡に制限がある場合には
ユーザに報告もできます

508
00:34:53,025 --> 00:34:56,196
再起動を勧めることも
できるでしょう

509
00:34:57,397 --> 00:35:02,936
情報を受信する端末では
対応の手助けができます

510
00:34:57,397 --> 00:35:02,936
情報を受信する端末では
対応の手助けができます

511
00:35:03,036 --> 00:35:06,273
取得する端末側の話をします

512
00:35:06,373 --> 00:35:09,843
撮影した画像は
World Mapと一緒に

513
00:35:09,943 --> 00:35:12,212
受信端末に送信します

514
00:35:12,312 --> 00:35:18,218
受信端末では 画像と
同じ場所を探してもらいます

515
00:35:19,787 --> 00:35:23,590
取得方法の次は
共有方法の説明です

516
00:35:24,892 --> 00:35:27,294
World Mapの取得には

517
00:35:27,394 --> 00:35:30,798
このインスタンスを使います

518
00:35:30,898 --> 00:35:33,000
これで取得可能です

519
00:35:34,635 --> 00:35:37,438
直列化可能クラスなので

520
00:35:37,571 --> 00:35:40,507
NSKeyedArchiverを使います

521
00:35:40,607 --> 00:35:42,876
データを並べるためです

522
00:35:43,577 --> 00:35:49,283
１人で利用する場合は
ディスクに保存もできます

523
00:35:49,616 --> 00:35:53,120
そうでない場合は
共有しましょう

524
00:35:53,287 --> 00:35:56,490
フレームワークを使います

525
00:35:56,857 --> 00:36:00,928
自動デバイスのような
優れた特徴があり

526
00:35:56,857 --> 00:36:00,928
自動デバイスのような
優れた特徴があり

527
00:36:01,028 --> 00:36:05,299
端末間での効率的な
データ通信を可能にします

528
00:36:05,532 --> 00:36:08,902
デベロッパWebサイトで

529
00:36:09,103 --> 00:36:13,774
ARKitでの使用例を
確認してみてください

530
00:36:15,809 --> 00:36:18,712
受信端末が
情報を受信したあと

531
00:36:19,279 --> 00:36:22,249
方向特定を
使うまでを説明します

532
00:36:22,349 --> 00:36:27,087
initialWorldMapを
設定するだけです

533
00:36:28,188 --> 00:36:32,726
セッションを起動すると
過去の情報を検索します

534
00:36:33,560 --> 00:36:38,232
同じ風景を指していないと
時間がかかります

535
00:36:38,699 --> 00:36:40,901
対応中かどうか知るには？

536
00:36:41,602 --> 00:36:44,505
追跡の状態で判断できます

537
00:36:44,605 --> 00:36:48,042
セッションを
開始するとすぐに

538
00:36:48,142 --> 00:36:51,812
対応を行うため
制限状態になるのです

539
00:36:52,846 --> 00:36:56,016
追跡データは取得可能ですが

540
00:36:56,116 --> 00:37:01,155
風景の基本は
最初に撮影したカメラです

541
00:36:56,116 --> 00:37:01,155
風景の基本は
最初に撮影したカメラです

542
00:37:02,623 --> 00:37:06,894
同じ風景を捉えると
対応が始まります

543
00:37:06,994 --> 00:37:12,432
追跡の状態が通常になり
風景が一致します

544
00:37:13,567 --> 00:37:17,104
Anchorが利用できるので

545
00:37:17,204 --> 00:37:19,540
仮想コンテンツを戻せます

546
00:37:21,742 --> 00:37:25,312
内部の処理に関する
注意事項です

547
00:37:25,546 --> 00:37:28,448
フィーチャーを
一致しているので

548
00:37:28,549 --> 00:37:32,119
風景の類似点が
必要になります

549
00:37:32,219 --> 00:37:35,522
つまり 対応したい場所の
風景です

550
00:37:36,089 --> 00:37:39,927
夜に同じ風景を
撮影しても失敗します

551
00:37:42,162 --> 00:37:46,199
World Mapの
保存やロードでの

552
00:37:46,300 --> 00:37:48,902
共有体験の説明でした

553
00:37:49,169 --> 00:37:51,638
次は 画像追跡の話です

554
00:37:54,775 --> 00:38:00,447
ARとは 現実世界に
仮想コンテンツを付与することです

555
00:37:54,775 --> 00:38:00,447
ARとは 現実世界に
仮想コンテンツを付与することです

556
00:38:00,547 --> 00:38:05,919
現実世界には
あらゆる物が存在します

557
00:38:06,086 --> 00:38:08,589
例えば 広告などです

558
00:38:08,755 --> 00:38:12,492
画像追跡で
物を認識することができ

559
00:38:12,593 --> 00:38:16,196
ARを体験することができます

560
00:38:17,798 --> 00:38:19,066
例を見ましょう

561
00:38:19,933 --> 00:38:23,804
２つの物が
同時に追跡されています

562
00:38:24,371 --> 00:38:27,608
左には ゾウが映っています

563
00:38:27,708 --> 00:38:31,044
ゾウの写真の上に
立っています

564
00:38:31,545 --> 00:38:35,449
右には 仮想画面が
映っています

565
00:38:36,116 --> 00:38:39,853
自由に動き回る物体を

566
00:38:40,053 --> 00:38:42,422
毎秒60フレームで
追跡します

567
00:38:43,924 --> 00:38:46,894
内部の処理を説明しましょう

568
00:38:47,361 --> 00:38:53,333
ゾウの画像を 右側の画像の中で
使いたいとします

569
00:38:54,201 --> 00:38:58,772
グレースケールにします
追跡に似た処理の話です

570
00:38:58,872 --> 00:39:03,911
参照画像と現在の風景で
興味深い点を追跡します

571
00:38:58,872 --> 00:39:03,911
参照画像と現在の風景で
興味深い点を追跡します

572
00:39:04,645 --> 00:39:09,717
そして フィーチャーを
参照イメージと一致させます

573
00:39:10,384 --> 00:39:15,522
幾何学と線形代数を用いると
現在の風景の

574
00:39:15,622 --> 00:39:19,459
最初の推定値は
十分に取得可能です

575
00:39:20,460 --> 00:39:21,762
まだ続けます

576
00:39:22,496 --> 00:39:26,833
正確で速い追跡を行うために

577
00:39:26,934 --> 00:39:29,636
高密度の追跡を進めます

578
00:39:29,903 --> 00:39:34,174
最初の推定値を使い
ピクセルを取り出します

579
00:39:35,442 --> 00:39:41,115
そして 右のような
長方形に変形させます

580
00:39:41,215 --> 00:39:43,150
再構成した画像です

581
00:39:43,484 --> 00:39:47,354
ピクセルを
長方形に変えています

582
00:39:47,821 --> 00:39:52,760
次に 再構成した画像と
参照画像を比較して

583
00:39:52,860 --> 00:39:55,796
下のような
エラー画像を作成

584
00:39:56,563 --> 00:40:01,869
画像の位置を最適化して
エラーを最小限にします

585
00:39:56,563 --> 00:40:01,869
画像の位置を最適化して
エラーを最小限にします

586
00:40:03,337 --> 00:40:07,708
つまり 非常に正確な
情報になるのです

587
00:40:09,276 --> 00:40:10,077
ありがとう

588
00:40:10,444 --> 00:40:13,180
毎秒60フレームで
追跡中です

589
00:40:15,215 --> 00:40:18,552
ARKitでの
処理方法を説明します

590
00:40:18,852 --> 00:40:19,853
これまでどおり―

591
00:40:21,054 --> 00:40:24,425
APIは単純に３段階です

592
00:40:24,525 --> 00:40:27,995
最初に 参照画像を
全て集めます

593
00:40:28,428 --> 00:40:32,966
それからARSessionを
２つの方法で設定

594
00:40:33,100 --> 00:40:37,237
ARWorldTrackingConfigurationを

595
00:40:37,337 --> 00:40:39,640
使うという方法が１つです

596
00:40:39,773 --> 00:40:44,778
iOS 12では独立型の
画像追跡を導入しました

597
00:40:46,613 --> 00:40:49,750
それから
ARImageAnchor型の

598
00:40:49,850 --> 00:40:52,653
結果を受け取ります

599
00:40:53,220 --> 00:40:58,258
３つの段階の１つめから
詳しく見ていきましょう

600
00:40:59,526 --> 00:41:05,432
アセットカタログで
参照画像を楽に追加できます

601
00:40:59,526 --> 00:41:05,432
アセットカタログで
参照画像を楽に追加できます

602
00:41:06,066 --> 00:41:10,738
AR Resource Groupを
作るだけです

603
00:41:11,505 --> 00:41:14,375
次に 画像の
物理的次元を設定

604
00:41:14,475 --> 00:41:17,878
右上のプロパティで
設定できます

605
00:41:19,213 --> 00:41:23,450
この設定は必須ですが
理由があります

606
00:41:24,385 --> 00:41:28,188
画像の配置を
物理的寸法にするので

607
00:41:28,288 --> 00:41:30,924
情報も
物理的寸法になるのです

608
00:41:31,091 --> 00:41:35,896
ARKitでは
全てがメートル表記です

609
00:41:37,297 --> 00:41:41,769
追跡した情報を
方向特定と融合する場合

610
00:41:41,869 --> 00:41:44,505
正しい設定が重要です

611
00:41:44,872 --> 00:41:49,376
設定により 画像と現実の
位置が一貫します

612
00:41:51,078 --> 00:41:54,681
参照画像の例を見ましょう

613
00:41:56,016 --> 00:41:58,385
２つの画像があります

614
00:41:58,519 --> 00:42:00,954
画像追跡でうまく機能します

615
00:41:58,519 --> 00:42:00,954
画像追跡でうまく機能します

616
00:42:01,054 --> 00:42:04,525
高い色調で
明暗差に優れています

617
00:42:04,625 --> 00:42:08,762
さらに 反復構造を
含みません

618
00:42:09,029 --> 00:42:13,233
うまく機能しない画像も
紹介します

619
00:42:13,367 --> 00:42:16,603
その例を右側に表示しました

620
00:42:17,071 --> 00:42:20,874
上の２つの例を見てみましょう

621
00:42:21,141 --> 00:42:25,379
よい画像には点が
多くあるのが分かります

622
00:42:25,479 --> 00:42:29,083
ヒストグラムも
うまく分散されています

623
00:42:29,249 --> 00:42:30,784
一方 悪い画像では―

624
00:42:31,752 --> 00:42:33,854
点の数が少なく

625
00:42:33,954 --> 00:42:37,024
ヒストグラムは非対称です

626
00:42:38,392 --> 00:42:43,397
画像の評価はXcodeから
直接入手できます

627
00:42:44,031 --> 00:42:47,901
画像をドラッグすると
分析されます

628
00:42:48,001 --> 00:42:52,773
エラーがあれば
起動前でも警告を出し

629
00:42:52,873 --> 00:42:54,842
フィードバックをします

630
00:42:55,576 --> 00:42:58,445
例えば ページ下にある

631
00:42:58,545 --> 00:43:01,515
雑誌の画像をクリックします

632
00:42:58,545 --> 00:43:01,515
雑誌の画像をクリックします

633
00:43:01,648 --> 00:43:06,520
Xcodeが警告を
出しているのが分かります

634
00:43:06,620 --> 00:43:09,456
画像が白すぎるためです

635
00:43:09,556 --> 00:43:14,361
さらに 反復構造が
含まれているようです

636
00:43:15,729 --> 00:43:19,767
別の例では 画像が
似すぎているため

637
00:43:19,867 --> 00:43:25,506
検出時に混乱を招く恐れが
あると警告が出ています

638
00:43:25,839 --> 00:43:30,077
この例は 同じ山の
２枚の画像で見られます

639
00:43:31,378 --> 00:43:33,781
警告の対処方法があります

640
00:43:33,881 --> 00:43:35,816
例えばこの絵です

641
00:43:35,916 --> 00:43:39,186
反復構造を含んでおり

642
00:43:39,286 --> 00:43:42,022
ヒストグラムの分散が変です

643
00:43:42,422 --> 00:43:45,993
この画像の特徴的な
領域を特定しましょう

644
00:43:46,093 --> 00:43:49,897
この場合 ページにある
写真のことです

645
00:43:50,030 --> 00:43:53,700
写真を切り取り
参照画像として使います

646
00:43:53,801 --> 00:43:57,337
すると 警告は全て消えます

647
00:43:57,438 --> 00:43:59,973
追跡の精度も上がります

648
00:44:01,875 --> 00:44:06,747
AR Resource Groupも
使えます

649
00:44:07,448 --> 00:44:10,284
より多くの画像の
検出が可能です

650
00:44:10,384 --> 00:44:14,054
検出できる画像を
最大で25におさえ

651
00:44:14,154 --> 00:44:17,057
効率的で 素早い反応を
保っています

652
00:44:17,858 --> 00:44:19,860
ただし いくつも作れます

653
00:44:19,960 --> 00:44:23,497
グループ間の
切り替えも可能です

654
00:44:23,964 --> 00:44:28,402
例えば 何百もの物がある
美術館で

655
00:44:28,502 --> 00:44:31,038
ARを構築するとします

656
00:44:31,905 --> 00:44:35,476
通常 美術品は
別々の部屋にあります

657
00:44:35,742 --> 00:44:38,845
そこで 部屋に置かれた
美術品を

658
00:44:38,946 --> 00:44:41,515
グループにまとめるのです

659
00:44:41,615 --> 00:44:44,084
部屋ごとにまとめましょう

660
00:44:44,184 --> 00:44:49,022
部屋を切り替えるために
中核となる場所を使います

661
00:44:50,891 --> 00:44:55,462
類似した物がないか
注意してください

662
00:44:56,530 --> 00:44:59,166
参照画像についてでした

663
00:44:59,533 --> 00:45:02,269
configurationの話をします

664
00:44:59,533 --> 00:45:02,269
configurationの話をします

665
00:45:02,936 --> 00:45:07,474
左は 独立型の
configurationです

666
00:45:07,574 --> 00:45:09,977
方向特定処理は
行いません

667
00:45:10,544 --> 00:45:13,046
元の画像も存在しないので

668
00:45:13,146 --> 00:45:16,850
カメラに写る
風景のみ使用します

669
00:45:18,185 --> 00:45:21,955
方向特定と画像追跡の
融合も可能です

670
00:45:23,524 --> 00:45:26,894
この場合
風景を理解できます

671
00:45:26,994 --> 00:45:30,397
例えば 平面検出と同じです

672
00:45:30,998 --> 00:45:34,668
どちらを使うのが
適切でしょうか？

673
00:45:34,968 --> 00:45:39,540
ARImageTrackingConfigurationは

674
00:45:39,707 --> 00:45:42,910
風景を構成するために
作られました

675
00:45:43,110 --> 00:45:45,178
左に例を映します

676
00:45:46,513 --> 00:45:50,050
教科書が置いてあります

677
00:45:50,450 --> 00:45:54,555
魅力的にするため
力学グラフを被せます

678
00:45:54,655 --> 00:45:57,124
今回は 正三角形になりました

679
00:45:57,791 --> 00:46:01,094
物体に適したグラフが
使われます

680
00:45:57,791 --> 00:46:01,094
物体に適したグラフが
使われます

681
00:46:01,395 --> 00:46:04,198
他の例も見てみましょう

682
00:46:04,465 --> 00:46:06,700
画像追跡が引き金になり

683
00:46:06,800 --> 00:46:10,170
実際の画像から
物が飛び出します

684
00:46:10,704 --> 00:46:15,275
ARWorldTrackingConfigurationで

685
00:46:15,375 --> 00:46:18,345
飛び出た物の
追跡ができます

686
00:46:20,013 --> 00:46:23,684
また モーションデータは
使いません

687
00:46:23,784 --> 00:46:26,854
つまり バスや
エレベーターでも

688
00:46:26,987 --> 00:46:29,556
画像追跡を
使うことができます

689
00:46:31,158 --> 00:46:34,094
では コードを
見てみましょう

690
00:46:35,128 --> 00:46:37,798
３つのステップがあります

691
00:46:37,898 --> 00:46:40,701
まず 画像を集めます

692
00:46:40,801 --> 00:46:45,105
特定のグループの画像を
全て集める

693
00:46:45,205 --> 00:46:48,375
便利なクラスもあります

694
00:46:48,475 --> 00:46:50,544
クラス名はRoom1です

695
00:46:51,812 --> 00:46:58,418
次にtrackingImagesを
クラスに設定します

696
00:46:58,519 --> 00:46:59,820
最後は実行です

697
00:47:00,821 --> 00:47:04,691
そして 処理結果を
受け取ります

698
00:47:04,792 --> 00:47:07,561
didUpdateなどを使います

699
00:47:07,828 --> 00:47:11,498
Anchorの型も確認可能です

700
00:47:12,533 --> 00:47:16,970
Anchorは物の位置や
方向の情報を

701
00:47:17,071 --> 00:47:18,572
保持しています

702
00:47:18,672 --> 00:47:21,241
自分が付けた名前が
使えるので

703
00:47:21,341 --> 00:47:25,045
どの画像が
検出されたか分かります

704
00:47:25,946 --> 00:47:27,881
Booleanでは―

705
00:47:28,215 --> 00:47:31,585
画像が追跡されているか
確認できます

706
00:47:33,787 --> 00:47:37,157
使用例は他にもあります

707
00:47:37,724 --> 00:47:41,461
画像検出や画像追跡を使えば

708
00:47:41,628 --> 00:47:43,997
他にもできることがあります

709
00:47:45,466 --> 00:47:49,670
例えば ２つの端末で
同じ画像を見る場合

710
00:47:50,037 --> 00:47:52,706
両端末で画像を検出できます

711
00:47:52,806 --> 00:47:56,477
座標系を
共有できるようになるので

712
00:47:56,577 --> 00:47:59,313
共有体験の
代替手段になります

713
00:48:01,749 --> 00:48:07,187
また 画像が実在する場所が
分かった場合の話をします

714
00:48:08,222 --> 00:48:12,292
例えば この公園の
場所が分かった場合

715
00:48:12,393 --> 00:48:16,964
画像追跡で 端末を
向ける位置が分かります

716
00:48:17,331 --> 00:48:21,802
端末の位置も現実世界に
関係しているので

717
00:48:21,902 --> 00:48:27,407
現実世界と同じように
方向を重ねることも可能です

718
00:48:30,010 --> 00:48:32,546
画像追跡の話は終わりです

719
00:48:32,646 --> 00:48:35,616
次は 物体検出の
説明をします

720
00:48:38,252 --> 00:48:41,488
画像追跡で 検出する物は

721
00:48:41,588 --> 00:48:44,758
平面的なオブジェクトでした

722
00:48:45,325 --> 00:48:48,162
物体検出では
3Dオブジェクトを

723
00:48:48,262 --> 00:48:51,098
検出できるようになりました

724
00:48:52,599 --> 00:48:57,971
ただし 動き回る物ではなく
静的な物を想定しています

725
00:48:58,772 --> 00:49:00,073
例を見ましょう

726
00:48:58,772 --> 00:49:00,073
例を見ましょう

727
00:49:01,008 --> 00:49:05,546
美術館にある
ネフェルティティの胸像です

728
00:49:05,646 --> 00:49:08,015
ARKitで検出しましょう

729
00:49:08,182 --> 00:49:13,520
それから 情報を
頭上に重ねてみましょう

730
00:49:15,456 --> 00:49:18,592
ARKitでの
物体検出では

731
00:49:18,692 --> 00:49:21,461
特定の物体について
話しています

732
00:49:21,562 --> 00:49:24,198
一般的な物体の話ではなく

733
00:49:24,298 --> 00:49:27,801
この胸像の話なので
注意してください

734
00:49:28,769 --> 00:49:31,305
では どう表示するのか？

735
00:49:31,572 --> 00:49:35,108
２つのステップがあります

736
00:49:35,208 --> 00:49:38,178
まず 物体をスキャンします

737
00:49:39,012 --> 00:49:44,485
デベロッパに関係する
スキャンについて説明します

738
00:49:44,785 --> 00:49:49,456
基本的には 検出で使用する
物体を作ります

739
00:49:51,525 --> 00:49:56,330
World Mapと
同様に表示されます

740
00:49:56,697 --> 00:50:02,302
胸像の3Dフィーチャーの点が
右に表示されています

741
00:49:56,697 --> 00:50:02,302
胸像の3Dフィーチャーの点が
右に表示されています

742
00:50:02,970 --> 00:50:07,441
スキャンに使用する
サンプルコードは

743
00:50:07,608 --> 00:50:10,811
ウェブサイトで入手可能です

744
00:50:11,678 --> 00:50:16,483
コードを実行して
得られる検出結果の品質は

745
00:50:16,583 --> 00:50:19,319
スキャンの品質に
左右されます

746
00:50:19,687 --> 00:50:25,325
では スキャン中に最高の
品質を得る方法を説明します

747
00:50:27,494 --> 00:50:32,766
サンプルをビルドして
実行したあとの表示です

748
00:50:33,967 --> 00:50:38,338
まず 物体の周辺に
スペースを見つけます

749
00:50:38,972 --> 00:50:44,311
フィーチャーから
境界ボックスを予測

750
00:50:44,812 --> 00:50:47,347
境界ボックスはドラッグで

751
00:50:47,448 --> 00:50:51,084
縮小 拡大の調整が可能です

752
00:50:53,053 --> 00:50:57,257
物体の周りを記録する時には

753
00:50:57,357 --> 00:51:01,428
特徴を見落とさないよう
注意してください

754
00:50:57,357 --> 00:51:01,428
特徴を見落とさないよう
注意してください

755
00:51:01,795 --> 00:51:05,532
２本指で物体を
回転することもできます

756
00:51:05,799 --> 00:51:11,638
境界ボックスが物体全体を
囲むようにしてください

757
00:51:13,273 --> 00:51:15,542
次は スキャンをします

758
00:51:17,344 --> 00:51:21,548
スキャンをする時は
物体の周囲を記録し

759
00:51:21,648 --> 00:51:26,120
物体の全体像を
捉えることが重要です

760
00:51:27,021 --> 00:51:31,491
どこまで捉えたかを
把握するために

761
00:51:31,592 --> 00:51:34,695
タイルが表示されます

762
00:51:34,828 --> 00:51:39,667
タイルの数は
上部に表示されます

763
00:51:40,334 --> 00:51:44,905
うまく検出できるように
時間をかけて

764
00:51:45,005 --> 00:51:48,042
物体の特徴を
捉えることが重要です

765
00:51:48,475 --> 00:51:50,544
近付いて細部を捉えます

766
00:51:50,644 --> 00:51:54,648
必ず あらゆる方向から
記録してください

767
00:51:56,350 --> 00:51:57,751
ご覧のとおりです

768
00:51:59,586 --> 00:52:04,091
物体を十分に捉えたら
次のステップです

769
00:51:59,586 --> 00:52:04,091
物体を十分に捉えたら
次のステップです

770
00:52:04,191 --> 00:52:07,394
ドラッグするだけで

771
00:52:07,561 --> 00:52:10,330
実際の物体に調整できます

772
00:52:10,497 --> 00:52:16,370
検出する際に使用できる
システムです

773
00:52:16,470 --> 00:52:21,074
仮想コンテンツとして
使えるように設置します

774
00:52:24,077 --> 00:52:28,215
ここまでで 物体の
全体像を捉えています

775
00:52:28,615 --> 00:52:31,151
つまり検出が可能です

776
00:52:31,652 --> 00:52:35,389
検出モードに
切り替わります

777
00:52:36,023 --> 00:52:41,395
このモードの使用を
推奨します

778
00:52:41,829 --> 00:52:46,667
違う視点から
物体を見ても

779
00:52:46,767 --> 00:52:49,469
物体が検出されていますね

780
00:52:50,571 --> 00:52:52,473
端末を遠ざけて

781
00:52:52,840 --> 00:52:55,442
別の角度から
物体を映します

782
00:52:55,809 --> 00:53:00,280
それでも問題なく
物体が検出できます

783
00:52:55,809 --> 00:53:00,280
それでも問題なく
物体が検出できます

784
00:53:01,448 --> 00:53:03,650
物体の周りを移動して

785
00:53:03,750 --> 00:53:06,186
明るさが
違う状態も試します

786
00:53:06,787 --> 00:53:12,025
おもちゃなどを
検出する場合に重要です

787
00:53:12,226 --> 00:53:15,829
どんな風景に重なるか
分からないからです

788
00:53:17,364 --> 00:53:21,535
違う環境で検出可能か
確認することも

789
00:53:21,969 --> 00:53:24,872
勧めています

790
00:53:25,572 --> 00:53:29,677
検出されなかった場合
再度スキャンします

791
00:53:29,910 --> 00:53:32,946
環境の明るさを
確認しましょう

792
00:53:34,014 --> 00:53:38,218
十分に明るい場所での
スキャンは重要です

793
00:53:38,418 --> 00:53:42,389
約500ルクスが最適でしょう

794
00:53:42,956 --> 00:53:48,328
それでも暗ければ
再度スキャンしてください

795
00:53:50,531 --> 00:53:55,869
検出の結果に満足できたら
Macにモデルをドロップ

796
00:53:56,136 --> 00:54:00,808
AR Resource Groupに
追加します

797
00:53:56,136 --> 00:54:00,808
AR Resource Groupに
追加します

798
00:54:02,910 --> 00:54:07,414
このシステムで
扱いやすい物の画像を

799
00:54:07,514 --> 00:54:09,316
左に表示しています

800
00:54:09,516 --> 00:54:15,022
固い物で質感がよく
特徴的な物質です

801
00:54:15,355 --> 00:54:18,826
扱いにくい物も
紹介しておきましょう

802
00:54:18,959 --> 00:54:22,529
例を右側に表示しました

803
00:54:22,629 --> 00:54:25,933
透明な金属や
光を反射する物は

804
00:54:26,033 --> 00:54:28,435
うまく扱えません

805
00:54:29,236 --> 00:54:33,774
ガラスなどの透明な物も
うまく検出できません

806
00:54:33,874 --> 00:54:37,444
置かれた場所で
外観が変わるためです

807
00:54:39,780 --> 00:54:41,582
スキャンの説明でした

808
00:54:41,682 --> 00:54:44,418
十分に明るい環境が必要です

809
00:54:44,918 --> 00:54:48,455
では 検出の処理を
見てみましょう

810
00:54:49,923 --> 00:54:55,429
APIが似ているので
親しみやすいかもしれません

811
00:54:55,529 --> 00:54:59,299
オブジェクトを集める
メソッドがあります

812
00:54:59,433 --> 00:55:02,236
今回も クラス内に
ありますね

813
00:54:59,433 --> 00:55:02,236
今回も クラス内に
ありますね

814
00:55:02,770 --> 00:55:06,840
クラスを確認するためには
オブジェクトを

815
00:55:06,940 --> 00:55:11,211
detectionObjectsに渡します

816
00:55:13,146 --> 00:55:17,718
セッションを開始すると
結果が返ってきます

817
00:55:18,252 --> 00:55:21,989
ARObjectAnchorも
確認します

818
00:55:22,323 --> 00:55:25,726
物の位置や方向の
情報を保持してます

819
00:55:26,894 --> 00:55:28,262
現実世界の情報です

820
00:55:28,662 --> 00:55:32,766
アセットカタログに
物体の名称があります

821
00:55:35,235 --> 00:55:39,506
物体検出と
World Mapの対応の

822
00:55:39,973 --> 00:55:43,010
類似性に気付いたでしょう

823
00:55:43,377 --> 00:55:44,812
相違点もあります

824
00:55:44,912 --> 00:55:47,214
物体検出の場合には

825
00:55:47,314 --> 00:55:51,251
物体の位置は
現実世界での位置を表します

826
00:55:51,418 --> 00:55:54,655
一方 World Mapは

827
00:55:54,755 --> 00:55:57,124
現在の情報に調整します

828
00:55:58,225 --> 00:56:01,829
物体検出では
複数の検出が可能です

829
00:55:58,225 --> 00:56:01,829
物体検出では
複数の検出が可能です

830
00:56:01,929 --> 00:56:06,767
テーブルの上などで
うまく機能します

831
00:56:07,067 --> 00:56:10,671
World Mapは
風景で機能します

832
00:56:12,339 --> 00:56:14,508
物体検出の説明は以上です

833
00:56:14,608 --> 00:56:16,643
今日の内容をまとめます

834
00:56:19,446 --> 00:56:23,350
位置特定は
端末の回転だけを追跡し

835
00:56:23,450 --> 00:56:26,854
静的な環境での
使用が可能です

836
00:56:27,854 --> 00:56:31,759
方向特定は位置を追跡
フィーチャーし

837
00:56:31,859 --> 00:56:36,563
元の位置情報を
端末に提供します

838
00:56:36,964 --> 00:56:40,801
風景全体の理解を
可能にしています

839
00:56:41,101 --> 00:56:43,737
そして 平面検出です

840
00:56:44,405 --> 00:56:49,643
垂直で平面な場所との
相互作用を実現し

841
00:56:49,743 --> 00:56:52,012
仮想コンテンツを付与します

842
00:56:53,981 --> 00:56:57,551
データの保存やロードで
一貫性のある―

843
00:56:57,651 --> 00:57:00,988
複数での共有体験を
紹介しました

844
00:56:57,651 --> 00:57:00,988
複数での共有体験を
紹介しました

845
00:57:01,455 --> 00:57:03,323
画像検出の話もしました

846
00:57:03,424 --> 00:57:07,027
毎秒60フレームで
追跡するシステムです

847
00:57:07,628 --> 00:57:11,665
画像検出で 物の全体を
検出できました

848
00:57:12,866 --> 00:57:16,870
今日の説明で
ARKitが持つ―

849
00:57:16,970 --> 00:57:22,443
追跡技術に関する 皆さんの
理解が深まればうれしいです

850
00:57:23,010 --> 00:57:26,313
品質についても話しましたね

851
00:57:26,413 --> 00:57:29,950
ぜひ ARKitを
活用してください

852
00:57:30,884 --> 00:57:34,755
詳しい情報は
デベロッパWebサイトをご覧ください

853
00:57:34,855 --> 00:57:37,925
ARKitのラボは明日です

854
00:57:38,292 --> 00:57:43,030
ARKitに関する質問に
お答えします

855
00:57:43,730 --> 00:57:47,434
ありがとうございました

856
00:57:47,668 --> 00:57:51,071
(拍手)